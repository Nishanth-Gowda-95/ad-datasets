(this["webpackJsonpmy-app"]=this["webpackJsonpmy-app"]||[]).push([[0],{131:function(e,t,a){},145:function(e,t,a){"use strict";a.r(t);var i=a(0),s=a.n(i),n=a(15),r=a.n(n),o=(a(131),a(7)),l=a(82),d=a(192),c=a(110),h=a(109),p=a(84),m=a(30),g=a(105),u=a.n(g),f=a(36),b=a(37),D=a(47),w=a(46),S=a(196),v=a(12),x=function(e){Object(D.a)(a,e);var t=Object(w.a)(a);function a(){return Object(f.a)(this,a),t.apply(this,arguments)}return Object(b.a)(a,[{key:"render",value:function(){return Object(v.jsx)(S.a,{label:"camera",color:"primary",size:"small",variant:"outlined",style:{display:this.props.visibility}})}}]),a}(i.Component),O=function(e){Object(D.a)(a,e);var t=Object(w.a)(a);function a(){return Object(f.a)(this,a),t.apply(this,arguments)}return Object(b.a)(a,[{key:"render",value:function(){return Object(v.jsx)(S.a,{label:"lidar",size:"small",variant:"outlined",style:{color:"mediumseagreen",border:"1px solid mediumseagreen",display:this.props.visibility}})}}]),a}(i.Component),C=function(e){Object(D.a)(a,e);var t=Object(w.a)(a);function a(){return Object(f.a)(this,a),t.apply(this,arguments)}return Object(b.a)(a,[{key:"render",value:function(){return Object(v.jsx)(S.a,{label:"radar",size:"small",variant:"outlined",style:{color:"maroon",border:"1px solid maroon",display:this.props.visibility}})}}]),a}(i.Component),y=function(e){Object(D.a)(a,e);var t=Object(w.a)(a);function a(){return Object(f.a)(this,a),t.apply(this,arguments)}return Object(b.a)(a,[{key:"render",value:function(){return Object(v.jsx)(S.a,{label:this.props.name,size:"small",variant:"outlined",style:{color:"peru",border:"1px solid peru",display:this.props.visibility}})}}]),a}(i.Component);var _=Object(m.a)((function(){return{root:{alignItems:"center",lineHeight:"24px",width:"100%",height:"100%",position:"relative",display:"flex","& .cellValue":{whiteSpace:"nowrap",overflow:"hidden",textOverflow:"ellipsis"}}}})),z=i.memo((function(e){var t=e.width,a=e.value,s=i.useRef(null),n=i.useRef(null),r=i.useRef(null),l=i.useState(null),d=Object(o.a)(l,2),m=d[0],g=d[1],u=_(),f=i.useState(!1),b=Object(o.a)(f,2),D=b[0],w=b[1],S=i.useState(!1),x=Object(o.a)(S,2),O=x[0],C=x[1];return i.useEffect((function(){if(D)return document.addEventListener("keydown",e),function(){document.removeEventListener("keydown",e)};function e(e){"Escape"!==e.key&&"Esc"!==e.key||w(!1)}}),[w,D]),Object(v.jsxs)("div",{ref:s,className:u.root,onMouseEnter:function(){var e,t=(e=r.current).scrollHeight>e.clientHeight||e.scrollWidth>e.clientWidth;C(t),g(n.current),w(!0)},onMouseLeave:function(){w(!1)},children:[Object(v.jsx)("div",{ref:n,style:{height:1,width:t,display:"block",position:"absolute",top:0}}),Object(v.jsx)("div",{ref:r,className:"cellValue",children:a}),O&&Object(v.jsx)(p.a,{open:D&&null!==m,anchorEl:m,style:{width:t,marginLeft:-17},children:Object(v.jsx)(h.a,{elevation:1,style:{minHeight:s.current.offsetHeight-3},children:Object(v.jsx)(c.a,{variant:"body2",style:{padding:8},children:a})})})]})}));function A(e){return Object(v.jsx)(z,{value:e.value?e.value.toString():"",width:e.colDef.width})}var k=[{field:"id",headerName:"Name",width:240,renderCell:function(e){return Object(v.jsx)("strong",{children:Object(v.jsx)(d.a,{variant:"contained",color:"primary",size:"small",style:{marginLeft:16},href:e.getValue(e.id,"href")||"",target:"_blank",children:e.getValue(e.id,"id")||""})})}},{field:"size_hours",headerName:"Size [h]",width:105,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"size_storage",headerName:"Size [GB]",width:120,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"frames",headerName:"Frames",width:110,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"numberOfScenes",headerName:"N\xb0 Scenes",width:130,align:"right",type:"number",sortable:!0,valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"samplingRate",headerName:"Sampling Rate [Hz]",width:180,hide:!0,type:"number"},{field:"lengthOfScenes",headerName:"Scene Length [s]",width:165,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"sensors",headerName:"Sensortypes",description:"types of sensors used in the dataset",type:"string",width:310,sortable:!1,renderCell:function(e){var t={camera:"none",lidar:"none",radar:"none",other1:"none",other2:"none",other3:"none"},a=function(e,t){try{var a=e.split(", ");return a.includes("camera")&&(t.camera="visible",a=a.filter((function(e){return"camera"!==e}))),a.includes("lidar")&&(t.lidar="visible",a=a.filter((function(e){return"lidar"!==e}))),a.includes("radar")&&(t.radar="visible",a=a.filter((function(e){return"radar"!==e}))),3===a.length?(t.other3="visible",t.other2="visible",t.other1="visible"):2===a.length?(t.other2="visible",t.other1="visible"):1===a.length&&""!==a[0]&&(t.other1="visible"),a}catch(i){}}(e.value,t);if(a)return Object(v.jsxs)("div",{children:[Object(v.jsx)(x,{visibility:t.camera}),Object(v.jsx)(O,{visibility:t.lidar}),Object(v.jsx)(C,{visibility:t.radar}),Object(v.jsx)(y,{visibility:t.other1,name:a[0]}),Object(v.jsx)(y,{visibility:t.other2,name:a[1]}),Object(v.jsx)(y,{visibility:t.other3,name:a[2]})]})}},{field:"sensorDetail",headerName:"Sensors - Details",description:"details on used sensors",type:"string",width:400,sortable:!1,hide:!0,renderCell:A},{field:"benchmark",headerName:"Benchmark",description:"This column has a value getter and is not sortable.",sortable:!1,width:360,hide:!0,renderCell:A},{field:"annotations",headerName:"Annotations",width:350,type:"String",sortable:!1,hide:!0,renderCell:A},{field:"location",headerName:"Location",width:380,hide:!0,type:"string",sortable:!1,renderCell:A},{field:"rawData",headerName:"Provide raw data",width:150,hide:!0,type:"string",sortable:!1,renderCell:A},{field:"licensing",headerName:"Licensing",width:350,sortable:!1,type:"string",renderCell:A},{field:"relatedDatasets",headerName:"Related Datasets",width:180,hide:!0,type:"string",sortable:!1,renderCell:A},{field:"publishDate",headerName:"Publish Date",width:150,hide:!0,type:"date",renderCell:A},{field:"lastUpdate",headerName:"Last Update",width:150,hide:!0,type:"date",renderCell:A},{field:"relatedPaper",headerName:"Related Paper",width:150,hide:!1,type:"string",renderCell:function(e){return Object(v.jsx)(d.a,{variant:"contained",color:"primary",size:"small",style:{marginLeft:16},href:e.getValue(e.id,"relatedPaper"),target:"_blank",children:Object(v.jsx)(u.a,{})})}}],I=[{id:"KITTI",href:"http://www.cvlibs.net/datasets/kitti/",size_hours:"6",size_storage:"180",frames:"",numberOfScenes:"50",samplingRate:"10",lengthOfScenes:"",sensors:"camera, lidar, gps/imu",sensorDetail:"2 greyscale cameras 1.4 MP, 2 color cameras 1.4 MP, 1 lidar 64 beams 360\xb0 10Hz, 1 inertial and GPS navigation system",benchmark:" stereo, optical flow, visual odometry, slam, 3d object detection, 3d object tracking",annotations:"3d bounding boxes",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 3.0",relatedDatasets:"Semantic KITTI, KITTI-360",publishDate:new Date("2012-3").toISOString().split("T")[0],lastUpdate:new Date("2021-2").toISOString().split("T")[0],relatedPaper:"http://www.cvlibs.net/publications/Geiger2013IJRR.pdf",location:"Karlsruhe, Germany",rawData:"Yes"},{id:"nuScenes",href:"https://www.nuscenes.org/",size_hours:"15",size_storage:"",frames:"1400000",numberOfScenes:"1000",samplingRate:"",lengthOfScenes:"20",sensors:"camera, lidar, radar, gps/imu",sensorDetail:"1x lidar 32 channels 360\xb0 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",benchmark:"3d object detection, tracking, trajectory (prediction), lidar segmentation, panoptic segmentation & tracking",annotations:"semantic category, attributes, 3d bounding boxes ",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",relatedDatasets:"nuImages",publishDate:new Date("2019-3").toISOString().split("T")[0],lastUpdate:new Date("2020-12").toISOString().split("T")[0],relatedPaper:"https://arxiv.org/pdf/1903.11027.pdf",location:"Boston, USA and Singapore",rawData:"Yes"},{id:"Oxford Robot Car",href:"https://robotcar-dataset.robots.ox.ac.uk/",size_hours:"210",size_storage:"23150",frames:"",numberOfScenes:"100",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, ins/gps",sensorDetail:"1x camera Bumblebee XB3 1280x960x3 16Hz, 3x camera Grasshopper2 1024x1024 12Hz, 2x lidar SICK LMS-151 270\xb0 50Hz, 1x lidar SICK LD-MRS 90\xb0 4 plane 12.5Hz, 1x NovAtel SPAN-CPT ALIGN 50Hz GPS+INS",benchmark:"",annotations:"",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",relatedDatasets:"Oxford Radar Robot Car",publishDate:new Date("2016-11").toISOString().split("T")[0],lastUpdate:new Date("2020-2").toISOString().split("T")[0],relatedPaper:"https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf",location:"Oxford, UK",rawData:"Yes"},{id:"Waymo Open Perception",href:"https://waymo.com/open/data/perception/",size_hours:"10.83",size_storage:"",frames:"390000",numberOfScenes:"1950",samplingRate:"10",lengthOfScenes:"20",sensors:"camera, lidar",sensorDetail:"5x cameras (front and sides) 1920x1280 & 1920x1040, 1x mid-range lidar, 4x short-range lidars",benchmark:"2d detection, 3d detection, 2d tracking, 3d tracking",annotations:"3d bounding boxes (lidar), 2d bounding boxes (camera)",licensing:"freely available for non-commercial purposes",relatedDatasets:"Waymo Open Motion",publishDate:new Date("2019-8").toISOString().split("T")[0],lastUpdate:new Date("2020-3").toISOString().split("T")[0],relatedPaper:"https://arxiv.org/pdf/1912.04838.pdf",location:"San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",rawData:"Yes"},{id:"Argoverse Motion Forecasting",href:"https://www.argoverse.org/",size_storage:"4.81",size_hours:"320",frames:"16227850",numberOfScenes:"324557",samplingRate:"10",lengthOfScenes:"5",sensors:"camera, lidar, gps",sensorDetail:"2x lidar 32 beam 40\xb0 10Hz, 7x ring cameras 1920x1200 combined 360\xb0 30Hz, 2x front-view facing stereo cameras 0.2986m baseline 2056x2464 5Hz",benchmark:"forecasting",annotations:"semantic vector map, rasterized map, trajectories",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",relatedDatasets:"Argoverse 3D Tracking",publishDate:new Date("2019-6").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1911.02620.pdf",location:"Miami and Pittsburgh, USA",rawData:"No"},{id:"Argoverse 3D Tracking",href:"https://www.argoverse.org/",size_storage:"254.4",size_hours:"1",frames:"44000",numberOfScenes:"113",samplingRate:"30",lengthOfScenes:"",sensors:"camera, lidar, gps",sensorDetail:"2x lidar 40\xb0 10Hz, 7x ring cameras 1920x1200 combined 360\xb0 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz",benchmark:"tracking",annotations:"semantic vector map, rasterized map, 3d bounding boxes",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",relatedDatasets:"Argoverse Motion Forecasting",publishDate:new Date("2019-6").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1911.02620.pdf",location:"Miami and Pittsburgh, USA",rawData:"Yes"},{id:"Semantic KITTI",href:"http://www.semantic-kitti.org/",size_storage:"",size_hours:"",frames:"43552",numberOfScenes:"21",samplingRate:"10",lengthOfScenes:"",sensors:"lidar",sensorDetail:"Velodyne HDL-64E from sequences of the odometry benchmark of the KITTI Vision Benchmark with 360\xb0 view",benchmark:"semantic segmentation, panoptic segmentation, 4D panoptic segmentation, moving object segmentation, semantic scene completion",annotations:"semantic segmentation",licensing:"Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) ",relatedDatasets:"KITTI",publishDate:new Date("2019-7").toISOString().split("T")[0],lastUpdate:new Date("2021-2").toISOString().split("T")[0],relatedPaper:"https://arxiv.org/abs/1904.01416.pdf",location:"Karlsruhe, Germany",rawData:"No"},{id:"ApolloScape",href:"http://apolloscape.auto/",size_hours:"100",size_storage:"",frames:"143906",numberOfScenes:"",samplingRate:"30",lengthOfScenes:"",sensors:"camera, lidar, imu/gnss",sensorDetail:"2x VUX-1HA laser scanners 360\xb0, 1x VMX-CS6 camera system, 1x measuring head with gnss/imu, 2x high frontal cameras 3384 \xd72710",benchmark:"2d image parsing, 3d car instance understanding, landmark segmentation, self-localization, trajectory prediction, 3d detection, 3d tracking, stereo",annotations:"high density 3d point cloud map, per-pixel, per-frame semantic image label, lane mark labelsemantic instance segmentation, geo-tagged",licensing:"freely available for non-commercial purposes",relatedDatasets:"",publishDate:new Date("2018-3").toISOString().split("T")[0],lastUpdate:new Date("2020-9").toISOString().split("T")[0],relatedPaper:"https://arxiv.org/pdf/1803.06184.pdf",location:"Beijing, Shanghai and Shenzhen, China",rawData:"Yes"},{id:"BDD100k",href:"https://www.bdd100k.com/",size_storage:"1800",size_hours:"1111",frames:"120000000",numberOfScenes:"100000",samplingRate:"30",lengthOfScenes:"40",sensors:"camera, gps/imu",sensorDetail:"crowd-sourced therefore no fixed setup, camera (720p) and gps/imu",benchmark:"object detection, instance segmentation, multiple object tracking, segmentation tracking, semantic segmentation, lane marking, drivable area, image tagging, imitation learning, domain adaption",annotations:"bounding boxes, instance segmentation, semantic segmentation, box tracking, semantic tracking, drivable area",licensing:"BSD 3-Clause",relatedDatasets:"",publishDate:new Date("2020-4").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1805.04687.pdf",location:"New York, Berkeley, San Francisco and Bay Area, USA",rawData:"Yes"},{id:"WildDash",href:"https://wilddash.cc/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"156",samplingRate:"",lengthOfScenes:"",sensors:"camera",sensorDetail:"various sources, e.g. YouTube",benchmark:"semantic segmentation, instance segmentation, panoptic segmentation",annotations:"semantic segmentation, instance segmentation",licensing:"CC-BY-NC 4.0 ",relatedDatasets:"",publishDate:new Date("2018-2").toISOString().split("T")[0],lastUpdate:new Date("2020-6").toISOString().split("T")[0],relatedPaper:"https://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf",location:"All over the world",rawData:"Yes"},{id:"Lyft Level5 Prediction",href:"https://level-5.global/data/prediction/",size_hours:"1118",size_storage:"",frames:"42500000",numberOfScenes:"170000",samplingRate:"10",lengthOfScenes:"25",sensors:"camera, lidar, radar",sensorDetail:"7 cameras with 360\xb0 view, 3 lidars with 40-64 channels at 10Hz, 5 radars",benchmark:"",annotations:"semantic map annotations, trajectories",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",relatedDatasets:"Lyft Level5 Perception",publishDate:new Date("2020-6").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://tinyurl.com/lyft-prediction-dataset",location:"Palo Alto, USA",rawData:"No"},{id:"Cityscapes 3D",href:"https://www.cityscapes-dataset.com/",size_hours:"",size_storage:"63.141",frames:"",numberOfScenes:"",samplingRate:"17",lengthOfScenes:"1.8",sensors:"camera, gps, thermometer",sensorDetail:"stereo cameras 22 cm baseline 17Hz, odometry from in-vehicle sensors & outside temperature & GPS tracks,",benchmark:"pixel-level semantic labeling, instance-level semantic labeling, panoptic semantic sabeling 3d vehicle detection",annotations:"dense semantic segmentation, instance segmentation for vehicles & people, 3d bounding boxes",licensing:"freely available for non-commercial purposes",relatedDatasets:"",publishDate:new Date("2016-2").toISOString().split("T")[0],lastUpdate:new Date("2020-10").toISOString().split("T")[0],relatedPaper:"https://arxiv.org/pdf/2006.07864.pdf",location:"50 cities in Germany and neighboring countries",rawData:"Yes"},{id:"Lyft Level5 Perception",href:"https://level-5.global/data/perception/",size_hours:"2.5",size_storage:"",frames:"",numberOfScenes:"366",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar",sensorDetail:"",benchmark:"",annotations:"3d bounding boxes, rasterised road geometry",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",relatedDatasets:"Lyft Level5 Prediction",publishDate:new Date("2019-7").toISOString().split("T")[0],lastUpdate:"",location:"Palo Alto, USA",rawData:"Yes"},{id:"nuImages",href:"https://www.nuscenes.org/nuimages",size_hours:"150",size_storage:"",frames:"1200000",numberOfScenes:"93000",samplingRate:"2",lengthOfScenes:"",sensors:"camera, lidar, radar, gps/imu",sensorDetail:"1x lidar 32 channels 360\xb0 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",benchmark:"",annotations:"instance masks, 2d bounding boxes, semantic segmentation masks, attribute annotations",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",relatedDatasets:"nuScenes",publishDate:new Date("2020-7").toISOString().split("T")[0],lastUpdate:"",location:"Boston, USA and Singapore",rawData:"Yes"},{id:"PandaSet",href:"https://pandaset.org/",size_hours:"0.23",size_storage:"",frames:"48000",numberOfScenes:"103",samplingRate:"",lengthOfScenes:"8",sensors:"camera, lidar, gps/imu",sensorDetail:"5x wide angle cameras 1920x1080 10Hz, 1x long focus camera 1920x1080 10Hz, 1x mechanical spinning LiDAR 64 channels 360\xb0 10Hz, 1x forward-facing LiDAR 150 channels 60\xb0 10Hz1x mechanical spinning LiDAR, 1x forward-facing LiDAR, 6x cameras, on-board GPS/IMU",benchmark:"",annotations:"3d bounding boxes, attributes, point cloud segmentation ",licensing:"Creative Commons Attribution 4.0 International Public (CC BY 4.0)",relatedDatasets:"",publishDate:new Date("2020-4").toISOString().split("T")[0],lastUpdate:"",location:"San Francisco and El Camina Real, USA",rawData:"Yes"},{id:"Waymo Open Motion",href:"https://waymo.com/open/data/motion/",size_hours:"574",size_storage:"",frames:"20670800",numberOfScenes:"103354",samplingRate:"10",lengthOfScenes:"20",sensors:"camera, lidar",sensorDetail:"5x cameras, 5x lidar, ",benchmark:"motion prediction, interaction prediction",annotations:"3d bounding boxes, 3d hd map information",licensing:"freely available for non-commercial purposes",relatedDatasets:"Waymo Open Perception",publishDate:new Date("2021-3").toISOString().split("T")[0],lastUpdate:new Date("2021-9").toISOString().split("T")[0],relatedPaper:"https://arxiv.org/pdf/2104.10133.pdf",location:"San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",rawData:"No"},{id:"openDD",href:"https://l3pilot.eu/data/opendd",size_storage:"",size_hours:"62.7",frames:"6771600",numberOfScenes:"501",samplingRate:"30",lengthOfScenes:"",sensors:"camera",sensorDetail:"DJI Phantom 4 3840\xd72160 camera drone",benchmark:"trajectory predictions",annotations:"2d bounding boxes, trajectories",licensing:"Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) ",relatedDatasets:"",publishData:new Date("2020-9").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/2007.08463.pdf",location:"Wolfsburg and Ingolstadt, Germany",rawData:"Yes"},{id:"RoadAnomaly21",href:"https://segmentmeifyoucan.com/datasets",size_storage:"0.05",size_hours:"",frames:"100",numberOfScenes:"100",samplingRate:"",lengthOfScenes:"",sensors:"camera",sensorDetail:"images from web resources 2048x1024 & 1280x720",benchmark:"anomaly segmentation",annotations:"semantic segmentation",licensing:'various, see "https://github.com/SegmentMeIfYouCan/road-anomaly-benchmark/blob/master/doc/RoadAnomaly/credits.txt" for detail',relatedDatasets:"RoadObstacle21",publishDate:new Date("2021-4").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/2104.14812.pdf",location:"",rawData:"Yes"},{id:"Comma2k19",href:"https://github.com/commaai/comma2k19",size_storage:"100",size_hours:"33.65",frames:"",numberOfScenes:"2019",samplingRate:"",lengthOfScenes:"60",sensors:"camera, radar, gnss/imu ",sensorDetail:"two different car types, 1x road-facing camera Sony IMX2984 20Hz, 1x gnss u-blox M8 chip5 10Hz, gyro and accelerometer data LSM6DS3 100Hz, magnetometer data AK09911 10Hz",benchmark:"",annotations:"",licensing:"MIT",relatedDatasets:"",publishDate:new Date("2018-12").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"http://export.arxiv.org/pdf/1812.05752",location:"California's 280 highway, USA",rawData:"Yes"},{id:"KITTI-360",href:"http://www.cvlibs.net/datasets/kitti-360/",size_storage:"",size_hours:"",frames:"400000",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, gps/imu",sensorDetail:"2x 180\xb0 fisheye camera, 1x 90\xb0 perspective stereo camera, 1x Velodyne HDL-64E & SICK LMS 200 laser scanning unit in pushbroom configuration",benchmark:"",annotations:"semantic instance segmentation",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 3.0",relatedDatasets:"KITTI",publishDate:new Date("2015-11").toISOString().split("T")[0],lastUpdate:new Date("2021-4").toISOString().split("T")[0],relatedPaper:"https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.pdf",location:"Karlsruhe, Germany",rawData:"Yes"},{id:"Fishyscapes",href:"https://fishyscapes.com/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"camera",sensorDetail:"based on the validation set of Cityscapes overlayed with anomalous objects and the original LostAndFound with extended pixel-wise annotations",benchmark:"anomaly detection for semantic segmentation",annotations:"semantic segmentation",licensing:"",relatedDatasets:"Cityscapes, LostAndFound",publishDate:new Date("2019-9").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1904.03215.pdf",location:"",rawData:"No"},{id:"LostAndFound",href:"http://www.6d-vision.com/lostandfounddataset",size_storage:"",size_hours:"",frames:"21040",numberOfScenes:"112",samplingRate:"",lengthOfScenes:"",sensors:"camera",sensorDetail:"stereo camera setup baseline 21cm 2048x1024",benchmark:"",annotations:"semantic segmentation",licensing:"freely available for non-commercial purposes",relatedDatasets:"",publishDate:new Date("2016-9").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1609.04653.pdf",location:"",rawData:"Yes"},{id:"KAIST Multi-Spectral Day/Night",href:"http://multispectral.kaist.ac.kr",size_storage:"",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"25",lengthOfScenes:"",sensors:"camera, lidar, gps/imu, thermal camera",sensorDetail:"2x PointGrey Flea3 RGB camera 1280 \xd7 960, 1x FLIR A655Sc thermal camera 640x480 50Hz, 1x Velodyne HDL-32E 3D LiDAR 360\xb0 32 beams 10Hz, 1x OXTS RT2002 gps/ins 100Hz",benchmark:"object detection, vision sensor enhancement, depth estimation, multi-spectral colorization",annotations:"dense depth map, bounding boxes",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 3.0",relatedDatasets:"",publishDate:new Date("2017-12").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8293689",location:"",rawData:"Yes"},{id:"A2D2",href:"https://www.a2d2.audi/a2d2/en.html",size_storage:"2300",size_hours:"",frames:"433833",numberOfScenes:"3",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, gps/imu",sensorDetail:"5x lidar 16 channels 360\xb0 10Hz, 1x front centre camera 1920x1208 30Hz, 5x surround cameras1920x1208 30Hz, vehicle bus data",benchmark:"",annotations:"semantic segmentation, point cloud segmentation, instance segmentation, 3d bounding boxes",licensing:"Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)",relatedDatasets:"",publishDate:new Date("2020-4").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/2004.06320.pdf",location:"Three cities in the south of Germany",rawData:"Yes"},{id:"Caltech Pedestrian",href:"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/",size_storage:"",size_hours:"10",frames:"1000000",numberOfScenes:"137",samplingRate:"30",lengthOfScenes:"60",sensors:"camera",sensorDetail:"1x camera 640x480 30Hz",benchmark:"pedestrian detection",annotations:"bounding boxes",licensing:"",relatedDatasets:"",publishDate:new Date("2010-3").toISOString().split("T")[0],lastUpdate:new Date("2019-1").toISOString().split("T")[0],relatedPaper:"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/files/CVPR09pedestrians.pdf",location:"Loa Angeles, USA",rawData:"Yes"},{id:"KAIST Urban",href:"https://irap.kaist.ac.kr/dataset/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"18",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, gps/imu",sensorDetail:"2x Velodyne VLP-16 16 channel lidar 360\xb0 10Hz, 2x SICK LMS-511 1 channel lidar 190\xb0 100Hz, 1x stereo camera 1280x560 10Hz",benchmark:"",annotations:"",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0",relatedDatasets:"",publishDate:new Date("2017-9").toISOString().split("T")[0],lastUpdate:new Date("2019-6").toISOString().split("T")[0],relatedPaper:"https://irap.kaist.ac.kr/dataset/papers/IJRR2019_dataset.pdf",location:"Seoul, Pangyo, Daejeon, Suwon and Dongtan, Korea",rawData:"Yes"},{id:"Udacity",href:"https://github.com/udacity/self-driving-car/",size_storage:"223",size_hours:"10",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, gps/imu",sensorDetail:"monocular color camera 1920x1200, velodyne 32 lidar, gps/imu",benchmark:"",annotations:"2d bounding boxes",licensing:"MIT",relatedDatasets:"",publishDate:new Date("2016-9").toISOString().split("T")[0],lastUpdate:"",location:"",rawData:"True"},{id:"Ford Autonomous Vehicle Dataset",href:"https://avdata.ford.com/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, gps/imu",sensorDetail:"4x HDL-32E Lidars, 4x Flea3 GigE Point Grey Cameras in stereo pairs (front & back) 80\xb0 15Hz,2x Flea3 GigE Point Grey Cameras (sides) 80\xb0 15Hz, 1x Flea3 GigE Point Grey Camera 40\xb0 7Hz, 1x Applanix POS LV gps/imu",benchmark:"",annotations:"3d point cloud maps, ground reflectivity map",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",relatedDatasets:"",publishDate:new Date("2020-3").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://s23.q4cdn.com/258866874/files/doc_downloads/2020/03/2003.07969.pdf",location:"Michigan, USA",rawData:"True"},{id:"INTERACTION dataset",href:"https://interaction-dataset.com/",size_storage:"",size_hours:"16.5",frames:"594588",numberOfScenes:"",samplingRate:"10",lengthOfScenes:"",sensors:"camera",sensorDetail:"drones & traffic cameras 3840x2160 30Hz downscaled to 10Hz",benchmark:"motion prediction",annotations:"2d bounding boxes, semantic map, motion/trajectories",licensing:"freely available for non-commercial purposes",relatedDatasets:"",publishDate:new Date("2019-9").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1910.03088.pdf",location:"USA, China, Germany and Bulgaria",rawData:"Yes"},{id:"MCity Data Collection",href:"https://arxiv.org/pdf/1912.06258.pdf",size_storage:"11000",size_hours:"50",frames:"",numberOfScenes:"255",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, radar, gps/imu",sensorDetail:"3x Velodyne Ultra Puck VLP-32C lidar 10Hz, 2x forward-facing cameras 30\xb0 1080P 30Hz,1x backward-facing camera 90\xb0 1080P 30Hz, 1x cabin pose camera 1280\xd71080 30Hz, 1x cabin head/eyeball camera 640P 30Hz, 1x Ibeo four beam LUX sensor 25Hz, 1x Delphi ESR 2.5 Radar 90\xb0 20Hz,1x NovAtel FlexPak6 with IMU-IGM-S1 and 4G cellular for RTK GPS single antenna 1Hz",benchmark:"",annotations:"semantic segmentation of objects, traffic lights, traffic signs, lanes",licensing:"",relatedDatasets:"",publishDate:new Date("2019-12").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1912.06258.pdf",location:"Ann Arbor, USA",rawData:"Yes"},{id:"Oxford Radar Robot Car",href:"https://oxford-robotics-institute.github.io/radar-robotcar-dataset/",size_storage:"4700",size_hours:"",frames:"",numberOfScenes:"32",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, radar, gps/imu",sensorDetail:"1 x Navtech CTS350-X Millimetre-Wave FMCW radar 4 Hz, 2 x Velodyne HDL-32E LIDAR 360\xb032 planes 20 Hz, 1 x Point Grey Bumblebee XB3 trinocular stereo camera 1280\xd7960\xd73 16 Hz 66\xb03 x Point Grey Grasshopper2 1024\xd71024 11.1 Hz 180\xb0, 2 x SICK LMS-151 2D LIDAR 270\xb0 50Hz, 1 x NovAtel SPAN-CPT ALIGN inertial and GPS navigation system 6 axis 50Hz,",benchmark:"",annotations:"ground truth data",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",relatedDatasets:"Oxford Robot Car",publishDate:new Date("2020-2").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1909.01300.pdf",location:"Oxford",rawData:"Yes"},{id:"NightOwls",href:"https://www.nightowls-dataset.org/",size_storage:"",size_hours:"5.17",frames:"279000",numberOfScenes:"40",samplingRate:"15",lengthOfScenes:"",sensors:"camera",sensorDetail:"1x industry standard 1024x640 camera",benchmark:"pedestrian detection, object detection",annotations:"bounding boxes, attributes, temporal tracking annotations",licensing:"freely available for non-commercial purposes",relatedDatasets:"",publishDate:new Date("2018-12").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/neumann18b.pdf",location:"Several cities across Europe",rawData:"Yes"},{id:"DDD 20",href:"https://sites.google.com/view/davis-driving-dataset-2020/home",size_storage:"1300",size_hours:"51",frames:"",numberOfScenes:"216",samplingRate:"",lengthOfScenes:"",sensors:"camera, car parameters",sensorDetail:"1x DAVIS346B 346x260 up to 50Hz, vehicle bus data",benchmark:"",annotations:"",licensing:"Creative Commons Attribution-ShareAlike 4.0 International",relatedDatasets:"DDD 17",publishDate:new Date("2020-2").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/2005.08605.pdf",location:"California, USA",rawData:"Yes"},{id:"H3D",href:"https://paperswithcode.com/dataset/h3d",size_storage:"",size_hours:"0.77",frames:"27721",numberOfScenes:"160",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, gps/imu",sensorDetail:"3x color PointGrey Grasshopper3 video cameras 1920x1200 90\xb0/80\xb0 30Hz, 1x Velodyne HDL-64E LiDAR 64 beams 360\xb0 10Hz, 1x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer 100Hz",benchmark:"",annotations:"3d bounding boxes",licensing:"",relatedDatasets:"",publishDate:new Date("2019-3").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1903.01568.pdf",location:"San Francisco Bay Area, USA",rawData:"Yes"},{id:"4Seasons",href:"https://www.4seasons-dataset.com/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"30",samplingRate:"30",lengthOfScenes:"",sensors:"camera, imu/rtk-gnss",sensorDetail:"2x cameras stereo baseline 30cm 800x400 (after cropping)",benchmark:"globally consistent reference poses",annotations:"",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",relatedDatasets:"",publishDate:new Date("2020-10").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/2009.06364.pdf",location:"",rawData:"Yes"},{id:"RadarScenes",href:"https://radar-scenes.com/",size_storage:"",size_hours:"4",frames:"",numberOfScenes:"158",samplingRate:"",lengthOfScenes:"",sensors:"camera, radar, odometry",sensorDetail:"4x 77 GHz series production automotive 60\xb0 radar sensor, 1x documentary camera",benchmark:"",annotations:"point-wise",licensing:"Creative Commons Attribution Non Commercial Share Alike 4.0 International",relatedDatasets:"",publishDate:new Date("2021-3").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/2104.02493.pdf",location:"",rawData:"Yes"},{id:"India Driving Dataset",href:"https://idd.insaan.iiit.ac.in/",size_storage:"",size_hours:"",frames:"10004",numberOfScenes:"182",samplingRate:"",lengthOfScenes:"",sensors:"camera",sensorDetail:"1080p & 720p stereo image",benchmark:"Pixel-Level Semantic Segmentation Task, Instance-Level Semantic Segmentation Task",annotations:"semantic segmentation",licensing:"",relatedDatasets:"",publishDate:new Date("2018-11").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://idd.insaan.iiit.ac.in/media/publications/idd-650.pdf",location:"Bangalore and Hyderabad, India",rawData:"Yes"},{id:"Synscapes",href:"https://7dlabs.com/synscapes-overview",size_storage:"",size_hours:"",frames:"25000",numberOfScenes:"25000",samplingRate:"",lengthOfScenes:"",sensors:"camera",sensorDetail:"RGB images in PNG format 1440x720 & upscaled version 2048x1024",benchmark:"",annotations:"2d bounding boxes, 3d bounding boxes, occlusion, truncation, semantic segmentation,instance segmentation, depth segmentation, scene metadata",licensing:"freely available for non-commercial purposes",relatedDatasets:"",publishDate:new Date("2018-10").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/1810.08705v1.pdf",location:"",rawData:""},{id:"RADIATE",href:"http://pro.hw.ac.uk/radiate/",size_storage:"",size_hours:"5",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"camera, lidar, radar, gps/imu",sensorDetail:"1x ZED stereo camera 672x376 15Hz, 1x Velodyne HDL-32e LiDAR 32 channel 360\xb0 10Hz, 1x Navtech CTS350-X radar 360\xb0, 1x Advanced Navigation Spatial Dual GPS/IMU",benchmark:"",annotations:"2d bounding boxes",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",relatedDatasets:"",publishDate:new Date("2020-10").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://arxiv.org/pdf/2010.09076.pdf",location:"",rawData:"Yes"},{id:"Bosch Small Traffic Lights Dataset",href:"https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset",size_storage:"",size_hours:"",frames:"13427",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"camera",sensorDetail:"raw 12bit HDR images with a red-clear-clear-blue filter 1280x720 & reconstructed 8-bit RGB color images 1280x720",benchmark:"",annotations:"bounding boxes, state",licensing:"freely available for non-commercial purposes",relatedDatasets:"",publishDate:new Date("2017-5").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989163",location:"",rawData:"Yes"},{id:"PepScenes",href:"https://github.com/huawei-noah/PePScenes",relatedPaper:"https://arxiv.org/pdf/2012.07773.pdf"},{id:"WZ-traffic dataset",href:"https://github.com/Fangyu0505/traffic-scene-recognition"},{id:"Nighttime Driving",href:"http://people.ee.ethz.ch/~daid/NightDriving/"},{id:"Cooperative Driving Dataset (CODD)",href:"https://github.com/eduardohenriquearnold/CODD"},{id:"AIODrive",href:"http://www.aiodrive.org/overview.html"},{id:"ROAD",href:"https://arxiv.org/abs/2102.11585"},{id:"ONCE",href:"https://once-for-auto-driving.github.io/index.html"},{id:"DriveU Traffic Light Dataset",href:"https://www.uni-ulm.de/en/in/driveu/projects/driveu-traffic-light-dataset/"},{id:"Bosch TL",href:"https://github.com/asimonov/Bosch-TL-Dataset"},{id:"nuPlan",href:"https://arxiv.org/abs/2106.11810"},{id:"Steet Hazards Dataset",href:"https://once-for-auto-driving.github.io/index.html"},{id:"JAAD",href:"https://paperswithcode.com/dataset/jaad"},{id:"RoadObstacle21",href:"https://segmentmeifyoucan.com/datasets"},{id:"Beyond PASCAL",href:"https://yuxng.github.io/Xiang_WACV_03242014.pdf"},{id:"The EuroCity Persons Dataset",href:"https://arxiv.org/abs/1805.07193"},{id:"Diverse Complexity Measures for Dataset Curation in Self-driving",href:"http://www.cs.toronto.edu/~sergio/publication/dataset/"},{id:"CADC",href:"http://cadcd.uwaterloo.ca/"},{id:"FAIRNets Dataset",href:"https://zenodo.org/record/3885249"},{id:"Diverse Complexity Measures for Dataset Curation in Self-driving",href:"https://arxiv.org/abs/2101.06554"},{id:"CARRADA Dataset",href:"https://arxiv.org/abs/2005.01456"},{id:"Astyx Dataset",href:"https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/astyx_dataset.html"},{id:"CNN-based Lidar Point Cloud De-Noising in Adverse Weather",href:"https://ieeexplore.ieee.org/document/8990038"},{id:"Playing for Data",href:"http://download.visinf.tu-darmstadt.de/data/from_games/index.html"},{id:"Talk2Car",href:"https://talk2car.github.io/"},{id:"A Parametric Top-View Representation of Complex Road Scenes",href:"https://www.nec-labs.com/~mas/BEV/"},{id:"DRIV100",href:"https://arxiv.org/pdf/2102.00150.pdf"},{id:"PointCloudDeNoising",href:"https://github.com/rheinzler/PointCloudDeNoising"},{id:"Cars Dataset",href:"https://ai.stanford.edu/~jkrause/cars/car_dataset.html"},{id:"CADP",href:"https://ppms.cit.cmu.edu/media/project_files/CADP_IEEE_Camera_Ready_Final.pdf"},{id:"CARLA100",href:"https://github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md",relatedPaper:"https://openaccess.thecvf.com/content_ICCV_2019/papers/Codevilla_Exploring_the_Limitations_of_Behavior_Cloning_for_Autonomous_Driving_ICCV_2019_paper.pdf"},{id:"VITRO",href:"https://vitro-testing.com/test-data/dashcam-annotations/"},{id:"UDrive Dataset",href:"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjqnfKIoK3uAhUOuaQKHZwcDEgQFjASegQIFBAC&url=https%3A%2F%2Ferticonetwork.com%2Fwp-content%2Fuploads%2F2017%2F12%2FUDRIVE-D41.1-UDrive-dataset-and-key-analysis-results-with-annotation-codebook.pdf&usg=AOvVaw17NgwnPrIal53hUYco9klG"},{id:"D^2 City",href:"https://arxiv.org/abs/1904.01975v1"},{id:"Semantic Segmentation Dataset for Urban Driving Scenes",href:"https://autonomous-driving.org/2018/07/15/semantic-segmentation-datasets-for-urban-driving-scenes/"},{id:"MIT-AVT Clustered Driving Scene Dataset",href:"https://ieeexplore.ieee.org/abstract/document/9304677/"},{id:"DDAD",href:"https://github.com/AdrienGaidon-TRI/DDAD"},{id:"RELLIS-3D Dataset",href:"https://arxiv.org/abs/2011.12954"},{id:"PolySync Dataset",href:"http://selfracingcars.com/blog/2016/7/26/polysync"},{id:"Scale AI",href:"https://techcrunch.com/2020/05/22/scale-ai-releases-free-lidar-dataset-to-power-self-driving-car-development/?guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAKGZ2LrvykvUJnpRzaXr-_6O48Af6iBGBjQWN7BZfLzH2l4dnkXnLk-oKzrjz7ioojEPaSDvwQ0TG3QuvY4e85v3r9I9pjNMaYpy5xGEmNFS9c3kr5nCthPREhtld9lFxWXquUMhJ2ReECLW3BWsibXJp7JOOHxCtw0XCg2GhRQa&guccounter=2"},{id:"DriveSeg",href:"https://news.mit.edu/2020/mit-toyota-release-visual-open-data-accelerate-autonomous-driving-research-0618"},{id:"KUL Belgium Traffic Sign dataset",href:"https://people.ee.ethz.ch/~timofter/traffic_signs/"},{id:"Brain4Cars",href:"http://brain4cars.com/"},{id:"Seasonal Variation Dataset",href:"http://www.cs.cmu.edu/~aayushb/localization/"},{id:"Bay Area Dataset",href:"http://www.cs.cmu.edu/~aayushb/localization/"},{id:"Illumination Changes in a day",href:"http://www.cs.cmu.edu/~aayushb/localization/"},{id:"comma.ai",href:"http://research.comma.ai/"},{id:"CULane Dataset",href:"https://xingangpan.github.io/projects/CULane.html"},{id:"DAVIS Driving Dataset",href:"https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub"},{id:"DBNet",href:"http://www.dbehavior.net/"},{id:"DIPLECS Autonomous Driving Datasets",href:"https://cvssp.org/data/diplecs/"},{id:"DR(eye)VE",href:"https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8"},{id:"SemanticPOSS",href:"http://www.poss.pku.edu.cn/semanticposs.html"},{id:"SemanticUSL",href:"https://unmannedlab.github.io/semanticusl"},{id:"ELEKTRA",href:"http://adas.cvc.uab.es/elektra/datasets/"},{id:"German Traffic Sign",href:"https://benchmark.ini.rub.de/"},{id:"HCI Challenging Stereo",href:"https://hci.iwr.uni-heidelberg.de/benchmarks/Challenging_Data_for_Stereo_and_Optical_Flow"},{id:"HD1K",href:"http://hci-benchmark.iwr.uni-heidelberg.de/"},{id:"Highway Work Zones",href:"http://www.andrew.cmu.edu/user/jonghole/workzone/data/"},{id:"LISA Traffic Sign Dataset",href:"http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html"},{id:"Malaga Stereo",href:"https://www.mrpt.org/MalagaUrbanDataset"},{id:"Malaga Laser Urban",href:"https://www.mrpt.org/MalagaUrbanDataset"},{id:"Mapillary Vistas",href:"https://www.mapillary.com/dataset/vistas"},{id:"NEXET",href:"https://blog.getnexar.com/https-medium-com-itayklein-intro-nexet-50e9b596d0e5"},{id:"Road Damage",href:"https://arxiv.org/pdf/1801.09454.pdf"},{id:"Ground Truth Stixel Dataset",href:"http://www.6d-vision.com/ground-truth-stixel-dataset"},{id:"Boxy",href:"https://boxy-dataset.com/boxy/"},{id:"TME Motorway",href:"http://cmp.felk.cvut.cz/data/motorway/"},{id:"TuSimple",href:"https://www.tusimple.com/"},{id:"UAH-DriveSet",href:"http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/"},{id:"Unsupervised Llamas",href:"https://unsupervised-llamas.com/llamas/"},{id:"Dense Depth for Autonomous Driving",href:"https://github.com/TRI-ML/DDAD"},{id:"Canadian Adverse Driving Conditions",href:"http://cadcd.uwaterloo.ca/"},{id:"Street Hazards",href:"https://github.com/hendrycks/anomaly-seg"},{id:"Astyx HiRes 2019",href:"https://www.astyx.com/fileadmin/redakteur/dokumente/Astyx_Dataset_HiRes2019_specification.pdf"},{id:"A*3D",href:"https://github.com/I2RDL2/ASTAR-3D"},{id:"camvid",href:"https://www.kaggle.com/carlolepelaars/camvid"},{id:"Daimler Urban Segmentation",href:"https://computervisiononline.com/dataset/1105138608"},{id:"VPGNet",href:"https://arxiv.org/abs/1710.06288"},{id:"Toronto 3D",href:"https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Tan_Toronto-3D_A_Large-Scale_Mobile_LiDAR_Dataset_for_Semantic_Segmentation_of_CVPRW_2020_paper.pdf"},{id:"Toronto City",href:"http://www.cs.toronto.edu/~byang/papers/Tcity_iccv17.pdf"},{id:"Synthia",href:"https://synthia-dataset.net/"},{id:"RANUS",href:"https://sites.google.com/site/gmchoe1/ranus"},{id:"One Thousand and One Hours",href:"https://arxiv.org/abs/2006.14480v2"},{id:"LIBRE",href:"https://arxiv.org/abs/2003.06129"},{id:"Stanford Track Collection",href:"https://cs.stanford.edu/people/teichman/stc/"},{id:"LiDAR-Video Driving Dataset",href:"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf"},{id:"WoodScape",href:"https://paperswithcode.com/dataset/woodscape"},{id:"Raincouver",href:"https://ieeexplore.ieee.org/document/7970170"},{id:"TRoM",href:"https://ieeexplore.ieee.org/document/8317749"},{id:"Caltech Lanes",href:"http://www.mohamedaly.info/datasets/caltech-lanes"},{id:"Complex Urban Dataset",href:"https://irap.kaist.ac.kr/dataset/"},{id:"CCSAD",href:"https://www.researchgate.net/publication/277476726_Towards_Ubiquitous_Autonomous_Driving_The_CCSAD_Dataset"},{id:"Street Learn",href:"https://paperswithcode.com/dataset/streetlearn"},{id:"Multi Vehicle Stereo Event Camera",href:"https://daniilidis-group.github.io/mvsec/"},{id:"AMUSE",href:"http://www.cvl.isy.liu.se/research/datasets/amuse/"},{id:"Cheddar Gorge Dataset",href:"https://www.researchgate.net/publication/228428941_The_Cheddar_Gorge_Data_Set"},{id:"The Annotated Laser Dataset",href:"https://journals.sagepub.com/doi/pdf/10.1177/0278364910389840"},{id:"DDD 17",href:"https://www.paperswithcode.com/dataset/ddd17"},{id:"BLVD",href:"https://paperswithcode.com/dataset/blvd"},{id:"FLIR Thermal Dataset",href:"https://www.flir.com/oem/adas/adas-dataset-form/"},{id:"Multispectral Object Detection",href:"https://deepai.org/publication/multispectral-object-detection-with-deep-learning"},{id:"CityPersons",href:"https://arxiv.org/abs/1702.05693"},{id:"Tsinghua Daimler Cyclist Detection",href:"http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html"},{id:"TDU Brussels Pedestrian",href:"https://www.researchgate.net/figure/Results-on-the-TUD-Brussels-Pedestrian-Dataset_fig13_321232691"},{id:"ETH Pedestrian",href:"https://paperswithcode.com/dataset/eth"},{id:"Daimler Pedestrian",href:"http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html"},{id:"GROUNDED Localizing Ground Penetrating Radar (LGPR) Evaluation Dataset",href:"https://lgprdata.com/"},{id:"highD",href:"https://www.highd-dataset.com/"},{id:"inD",href:"https://www.ind-dataset.com/"},{id:"rounD",href:"https://www.round-dataset.com/"},{id:"TAF-BW",href:"https://github.com/fzi-forschungszentrum-informatik/test-area-autonomous-driving-dataset"},{id:"Road Scene Graph",href:"https://github.com/TianYafu/road-status-graph-dataset"},{id:"R3 Driving Dataset",href:"https://github.com/rllab-snu/R3-Driving-Dataset",relatedPaper:"https://arxiv.org/pdf/2109.07995.pdf"},{id:"EISATS",href:"https://ccv.wordpress.fos.auckland.ac.nz/eisats/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"",sensorDetail:"",benchmark:"",annotations:"",licensing:"",relatedDatasets:"",publishDate:" ",lastUpdate:"",relatedPaper:""},{id:"Ford CAMPUS",href:"https://www.researchgate.net/publication/220122924_Ford_Campus_vision_and_lidar_data_set",size_storage:"",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"",sensorDetail:"",benchmark:"",annotations:"",licensing:"",relatedDatasets:"",publishDate:" ",lastUpdate:"",relatedPaper:""},{id:"Argoverse Stereo",href:"https://www.argoverse.org/data.html#stereo-link",size_storage:"14.2",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"",sensorDetail:"",benchmark:"stereo",annotations:"",licensing:"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",relatedDatasets:"Argoverse 3D Tracking",publishDate:new Date("2021-4").toISOString().split("T")[0],lastUpdate:"",relatedPaper:""},{id:"uniD",href:"https://www.unid-dataset.com/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"",sensorDetail:"",benchmark:"",annotations:"",licensing:"",relatedDatasets:"",publishDate:" ",lastUpdate:"",relatedPaper:""},{id:"exiD",href:"https://www.exid-dataset.com/",size_storage:"",size_hours:"",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"",sensorDetail:"",benchmark:"",annotations:"",licensing:"",relatedDatasets:"",publishDate:" ",lastUpdate:"",relatedPaper:""},{id:"The USyd Campus Dataset",href:"http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/",size_storage:"",size_hours:"40",frames:"",numberOfScenes:"",samplingRate:"",lengthOfScenes:"",sensors:"6 cameras, lidar, gps, imu, wheel encoders",sensorDetail:"GMSL cameras 30, 60 Hz, VLP-16 10 Hz",benchmark:"",annotations:"Semantic Segmentation",licensing:"",relatedDatasets:"",publishDate:new Date("2020-6-5").toISOString().split("T")[0],lastUpdate:"",relatedPaper:"https://ieeexplore.ieee.org/document/9109704"},{id:"SemKITTI-DVPS",href:"https://github.com/joe-siyuan-qiao/ViP-DeepLab",relatedDatasets:"SemanticKITTI ",relatedPaper:"https://arxiv.org/pdf/2012.05258",location:"Karlsruhe, Germany",rawData:"Yes"},{id:"Cityscapes-DVPS",href:"https://github.com/joe-siyuan-qiao/ViP-DeepLab",relatedDatasets:"Cityscapes",relatedPaper:"https://arxiv.org/pdf/2012.05258",rawData:"Yes"},{id:"IDDA",href:"https://idda-dataset.github.io/home/",relatedPaper:"https://arxiv.org/pdf/2004.08298.pdf",benchmark:"semantic segmentation",rawData:"Yes"},{id:"UTBM EU LTD",href:"https://epan-utbm.github.io/utbm_robocar_dataset/",relatedPaper:"https://arxiv.org/pdf/1909.03330.pdf",rawData:"Yes",publishDate:new Date("2020-8").toISOString().split("T")[0]}],P=a(98),T=a(55),R=a(106),j=a.n(R),N=a(193),L=Object(P.a)((function(e){return{note:{position:"absolute",right:"0.5%",top:.01*window.innerHeight,fontSize:.007*window.innerWidth,border:"1px solid white",borderRadius:"5px",paddingLeft:"0.25%",paddingRight:"0.25%",color:"white",textTransform:"none"},lastUpdate:{position:"absolute",left:"0.5%",top:.01*window.innerHeight,fontSize:.007*window.innerWidth,border:"1px solid white",borderRadius:"5px",padding:"0.5%",color:"#3f51b5",textTransform:"none",backgroundColor:"white"},title:{display:"block",textAlign:"center",float:"center",fontSize:.02*window.innerWidth,paddingTop:".2%"},subTitle:{textAlign:"center",float:"center",fontSize:.01*window.innerWidth,position:"relative"},customDatagrid:{height:.9*window.innerHeight},headBand:{width:window.innerWidth,height:.1*window.innerHeight,backgroundColor:"#3f51b5",color:"white"}}}));function H(){var e=i.useState({height:window.innerHeight,width:window.innerWidth}),t=Object(o.a)(e,2)[1];i.useEffect((function(){function e(){t({height:window.innerHeight,width:window.innerWidth})}return window.addEventListener("resize",e),function(t){window.removeEventListener("resize",e)}}));var a=L(),s=i.useState(10),n=Object(o.a)(s,2),r=n[0],d=n[1],h="visible",p="hidden";return!1===Object(N.a)(j()({minWidth:.535*window.screen.width}))&&(h="hidden",p="visible"),Object(v.jsxs)("div",{style:{width:"100%",height:window.innerHeight},children:[Object(v.jsxs)("div",{className:a.headBand,style:{width:window.innerWidth},children:[Object(v.jsx)(c.a,{className:a.title,children:"ad-datasets"}),Object(v.jsx)(c.a,{className:a.subTitle,children:"Complete* and curated list of autonomous driving related datasets"})]}),Object(v.jsxs)(T.a,{className:a.note,target:"_blank",style:{visibility:h},href:"https://github.com/daniel-bogdoll/ad-datasets",children:["*Could not find your dataset? ",Object(v.jsx)("br",{})," Simply create a pull request ;)"]}),Object(v.jsx)(T.a,{className:a.note,target:"_blank",style:{visibility:p},href:"https://github.com/daniel-bogdoll/ad-datasets",children:"*GitHub"}),Object(v.jsxs)(c.a,{className:a.lastUpdate,children:["Last Update: ","2022-02-07"]}),Object(v.jsx)(l.a,{rows:I,columns:k,components:{Toolbar:l.b},disableColumnMenu:!0,sortingOrder:["desc","asc"],pageSize:r,onPageSizeChange:function(e){return d(e)},rowsPerPageOptions:[10,25,50],pagination:!0,columnBuffer:3,className:a.customDatagrid,style:{width:Window.innerWidth}})]})}var U=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,198)).then((function(t){var a=t.getCLS,i=t.getFID,s=t.getFCP,n=t.getLCP,r=t.getTTFB;a(e),i(e),s(e),n(e),r(e)}))};r.a.render(Object(v.jsx)(s.a.StrictMode,{children:Object(v.jsx)(H,{})}),document.getElementById("root")),U()}},[[145,1,2]]]);
//# sourceMappingURL=main.b9ed5e43.chunk.js.map