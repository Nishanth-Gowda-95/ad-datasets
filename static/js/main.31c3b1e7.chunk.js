(this["webpackJsonpmy-app"]=this["webpackJsonpmy-app"]||[]).push([[0],{135:function(e,t,a){},146:function(e,t,a){"use strict";a.r(t);var i=a(0),n=a.n(i),s=a(15),o=a.n(s),r=(a(135),a(8)),l=a(86),c=a(204),p=a(112),d=a(111),m=a(88),u=a(34),h=a(32),g=a(33),f=a(37),b=a(36),D=a(209),S=a(208),C=a(7),v=function(e){Object(f.a)(a,e);var t=Object(b.a)(a);function a(){return Object(h.a)(this,a),t.apply(this,arguments)}return Object(g.a)(a,[{key:"render",value:function(){return Object(C.jsx)(D.a,{label:"camera",color:"primary",size:"small",variant:"outlined",style:{display:this.props.visibility}})}}]),a}(i.Component),x=function(e){Object(f.a)(a,e);var t=Object(b.a)(a);function a(){return Object(h.a)(this,a),t.apply(this,arguments)}return Object(g.a)(a,[{key:"render",value:function(){return Object(C.jsx)(D.a,{label:"lidar",size:"small",variant:"outlined",style:{color:"mediumseagreen",border:"1px solid mediumseagreen",display:this.props.visibility}})}}]),a}(i.Component),y=function(e){Object(f.a)(a,e);var t=Object(b.a)(a);function a(){return Object(h.a)(this,a),t.apply(this,arguments)}return Object(g.a)(a,[{key:"render",value:function(){return Object(C.jsx)(D.a,{label:"radar",size:"small",variant:"outlined",style:{color:"maroon",border:"1px solid maroon",display:this.props.visibility}})}}]),a}(i.Component),w=function(e){Object(f.a)(a,e);var t=Object(b.a)(a);function a(){return Object(h.a)(this,a),t.apply(this,arguments)}return Object(g.a)(a,[{key:"render",value:function(){return Object(C.jsx)(D.a,{label:this.props.name,size:"small",variant:"outlined",style:{color:"peru",border:"1px solid peru",display:this.props.visibility}})}}]),a}(i.Component),O=function(e){Object(f.a)(a,e);var t=Object(b.a)(a);function a(){return Object(h.a)(this,a),t.apply(this,arguments)}return Object(g.a)(a,[{key:"render",value:function(){return Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"flex-start",border:"1px solid white",borderRadius:"10px",marginTop:"10px",backgroundColor:"rgba(0, 0, 0, 0.2)"},children:[Object(C.jsx)(S.a,{style:{height:"3px",width:"3px",background:"red",padding:"0px",opacity:"0.3",margin:"1px",backgroundColor:"rgba(0, 0, 0, 0.5)"}}),Object(C.jsx)(S.a,{style:{height:"3px",width:"3px",background:"yellow",padding:"0px",marginRight:"1px",opacity:"0.3",margin:"1px",backgroundColor:"rgba(0, 0, 0, 0.5)"}}),Object(C.jsx)(S.a,{color:"success",style:{height:"3px",width:"3px",padding:"0px",marginRight:"1px",margin:"1px"}})]})}}]),a}(i.Component),z=function(e){Object(f.a)(a,e);var t=Object(b.a)(a);function a(){return Object(h.a)(this,a),t.apply(this,arguments)}return Object(g.a)(a,[{key:"render",value:function(){return Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"flex-start",border:"1px solid white",borderRadius:"10px",marginTop:"10px",backgroundColor:"rgba(0, 0, 0, 0.2)"},children:[Object(C.jsx)(S.a,{color:"success",style:{height:"3px",width:"3px",padding:"0",opacity:"0.3",margin:"1px",backgroundColor:"rgba(0, 0, 0, 0.5)"}}),Object(C.jsx)(S.a,{style:{height:"3px",width:"3px",background:"yellow",padding:"0",margin:"1px"}}),Object(C.jsx)(S.a,{style:{height:"3px",width:"3px",background:"red",padding:"0",opacity:"0.3",margin:"1px",backgroundColor:"rgba(0, 0, 0, 0.5)"}})]})}}]),a}(i.Component),A=function(e){Object(f.a)(a,e);var t=Object(b.a)(a);function a(){return Object(h.a)(this,a),t.apply(this,arguments)}return Object(g.a)(a,[{key:"render",value:function(){return Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"flex-start",border:"1px solid white",borderRadius:"10px",marginTop:"10px",backgroundColor:"rgba(0, 0, 0, 0.2)"},children:[Object(C.jsx)(S.a,{style:{height:"3px",width:"3px",background:"red",padding:"0",margin:"1px"}}),Object(C.jsx)(S.a,{style:{height:"3px",width:"3px",background:"yellow",padding:"0",margin:"1px",opacity:"0.3",backgroundColor:"rgba(0, 0, 0, 0.5)"}}),Object(C.jsx)(S.a,{color:"success",style:{height:"3px",width:"3px",padding:"0",margin:"1px",opacity:"0.3",backgroundColor:"rgba(0, 0, 0, 0.5)"}})]})}}]),a}(i.Component);var _=Object(u.a)((function(){return{root:{alignItems:"center",lineHeight:"24px",width:"100%",height:"100%",position:"relative",display:"flex","& .cellValue":{whiteSpace:"nowrap",overflow:"hidden",textOverflow:"ellipsis"}}}})),P=i.memo((function(e){var t=e.width,a=e.value,n=i.useRef(null),s=i.useRef(null),o=i.useRef(null),l=i.useState(null),c=Object(r.a)(l,2),u=c[0],h=c[1],g=_(),f=i.useState(!1),b=Object(r.a)(f,2),D=b[0],S=b[1],v=i.useState(!1),x=Object(r.a)(v,2),y=x[0],w=x[1];return i.useEffect((function(){if(D)return document.addEventListener("keydown",e),function(){document.removeEventListener("keydown",e)};function e(e){"Escape"!==e.key&&"Esc"!==e.key||S(!1)}}),[S,D]),Object(C.jsxs)("div",{ref:n,className:g.root,onMouseEnter:function(){var e,t=(e=o.current).scrollHeight>e.clientHeight||e.scrollWidth>e.clientWidth;w(t),h(s.current),S(!0)},onMouseLeave:function(){S(!1)},children:[Object(C.jsx)("div",{ref:s,style:{height:1,width:t,display:"block",position:"absolute",top:0}}),Object(C.jsx)("div",{ref:o,className:"cellValue",children:a}),y&&Object(C.jsx)(m.a,{open:D&&null!==u,anchorEl:u,style:{width:t,marginLeft:-17},children:Object(C.jsx)(d.a,{elevation:1,style:{minHeight:n.current.offsetHeight-3},children:Object(C.jsx)(p.a,{variant:"body2",style:{padding:8},children:a})})})]})}));function k(e){return"-"!==e.value?Object(C.jsx)(P,{value:e.value?e.value.toString():"",width:e.colDef.width}):""}var R=[{field:"id",headerName:"Name",width:240,renderCell:function(e){return"complete"===e.getValue(e.id,"completionStatus")?Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"row",alignItems:"flex-start"},children:[Object(C.jsx)(O,{}),Object(C.jsx)("strong",{children:Object(C.jsx)(c.a,{variant:"contained",color:"primary",size:"small",style:{marginLeft:6},href:e.getValue(e.id,"href")||"",target:"_blank",children:e.getValue(e.id,"id")||""})})]}):"partially Complete"===e.getValue(e.id,"completionStatus")?Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"row",alignItems:"flex-start"},children:[Object(C.jsx)(z,{}),Object(C.jsx)("strong",{children:Object(C.jsx)(c.a,{variant:"contained",color:"primary",size:"small",style:{marginLeft:6},href:e.getValue(e.id,"href")||"",target:"_blank",children:e.getValue(e.id,"id")||""})})]}):"incomplete"===e.getValue(e.id,"completionStatus")?Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"row",alignItems:"flex-start"},children:[Object(C.jsx)(A,{}),Object(C.jsx)("strong",{children:Object(C.jsx)(c.a,{variant:"contained",color:"primary",size:"small",style:{marginLeft:6},href:e.getValue(e.id,"href")||"",target:"_blank",children:e.getValue(e.id,"id")||""})})]}):void 0}},{field:"citationCount",headerName:"N\xb0 Citations",width:150,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"size_hours",headerName:"Size [h]",width:105,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"size_storage",headerName:"Size [GB]",width:120,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"frames",headerName:"Frames",width:110,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"numberOfScenes",headerName:"N\xb0 Scenes",width:130,align:"right",type:"number",sortable:!0,valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"samplingRate",headerName:"Sampling Rate [Hz]",width:180,hide:!0,type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"lengthOfScenes",headerName:"Scene Length [s]",width:165,align:"right",type:"number",valueFormatter:function(e){var t=Number(e.value).toLocaleString();return"0"!==t&&"NaN"!==t?"".concat(t):""}},{field:"sensors",headerName:"Sensortypes",description:"types of sensors used in the dataset",type:"string",width:310,sortable:!1,renderCell:function(e){var t={camera:"none",lidar:"none",radar:"none",other1:"none",other2:"none",other3:"none"},a=function(e,t){try{var a=e.split(", ");return a.includes("camera")&&(t.camera="visible",a=a.filter((function(e){return"camera"!==e}))),a.includes("lidar")&&(t.lidar="visible",a=a.filter((function(e){return"lidar"!==e}))),a.includes("radar")&&(t.radar="visible",a=a.filter((function(e){return"radar"!==e}))),3===a.length?(t.other3="visible",t.other2="visible",t.other1="visible"):2===a.length?(t.other2="visible",t.other1="visible"):1===a.length&&""!==a[0]&&(t.other1="visible"),a}catch(i){}}(e.value,t);if(a)return Object(C.jsxs)("div",{children:[Object(C.jsx)(v,{visibility:t.camera}),Object(C.jsx)(x,{visibility:t.lidar}),Object(C.jsx)(y,{visibility:t.radar}),Object(C.jsx)(w,{visibility:t.other1,name:a[0]}),Object(C.jsx)(w,{visibility:t.other2,name:a[1]}),Object(C.jsx)(w,{visibility:t.other3,name:a[2]})]})}},{field:"sensorDetail",headerName:"Sensors - Details",description:"details on used sensors",type:"string",width:400,sortable:!1,hide:!0,renderCell:k},{field:"benchmark",headerName:"Benchmark",description:"This column has a value getter and is not sortable.",sortable:!1,width:360,hide:!0,renderCell:k},{field:"annotations",headerName:"Annotations",width:350,type:"String",sortable:!1,hide:!0,renderCell:k},{field:"location",headerName:"Location",width:380,hide:!0,type:"string",sortable:!1,renderCell:k},{field:"rawData",headerName:"Provide raw data",width:150,hide:!0,type:"string",sortable:!1,renderCell:k},{field:"licensing",headerName:"Licensing",width:350,sortable:!1,type:"string",renderCell:k},{field:"relatedDatasets",headerName:"Related Datasets",width:180,hide:!0,type:"string",sortable:!1,renderCell:k},{field:"publishDate",headerName:"Publish Date",width:150,hide:!0,type:"date",renderCell:k},{field:"lastUpdate",headerName:"Last Update",width:150,hide:!0,type:"date",renderCell:k},{field:"relatedPaper",headerName:"Related Paper",width:150,hide:!1,type:"string",renderCell:function(e){return Object(C.jsx)("strong",{children:Object(C.jsx)(c.a,{variant:"contained",color:"primary",size:"small",style:{marginLeft:16},href:e.getValue(e.id,"relatedPaper")||"",target:"_blank",children:e.getValue(e.id,"paperTitle")||""})})}}],I=a(102),T=a(51),j=a(108),N=a.n(j),L=a(205),U=a(85),H=a(85),V=0,G=0,M=0;!function(){for(var e in H)console.log(H[e].id),console.log(H[e].completionStatus),"complete"===H[e].completionStatus?V++:"partially Complete"===H[e].completionStatus?G++:"incomplete"===H[e].completionStatus&&M++,console.log("Complete: ",V)}();var E=Object(I.a)((function(e){return{note:{position:"absolute",right:"0.5%",top:.01*window.innerHeight,fontSize:.007*window.innerWidth,border:"1px solid white",borderRadius:"5px",paddingLeft:"0.25%",paddingRight:"0.25%",color:"white",textTransform:"none"},lastUpdate:{position:"absolute",left:"0.5%",top:.01*window.innerHeight,fontSize:.007*window.innerWidth,border:"1px solid white",borderRadius:"5px",padding:"0.5%",color:"#3f51b5",textTransform:"none",backgroundColor:"white"},title:{display:"block",textAlign:"center",float:"center",fontSize:.02*window.innerWidth,paddingTop:".2%"},subTitle:{textAlign:"center",float:"center",fontSize:.01*window.innerWidth,position:"relative"},customDatagrid:{height:.9*window.innerHeight},headBand:{width:window.innerWidth,height:.1*window.innerHeight,backgroundColor:"#3f51b5",color:"white"},legend:{bottom:"1%",left:"1%",backgroundColor:"white",position:"absolute",zIndex:"-1",fontSize:"10px",display:"flex",flexDirection:"row",alignItems:"flex-start"}}}));function F(){var e=i.useState({height:window.innerHeight,width:window.innerWidth}),t=Object(r.a)(e,2)[1];i.useEffect((function(){function e(){t({height:window.innerHeight,width:window.innerWidth})}return window.addEventListener("resize",e),function(t){window.removeEventListener("resize",e)}}));var a=E(),n=i.useState(10),s=Object(r.a)(n,2),o=s[0],c=s[1],d="visible",m="hidden";return!1===Object(L.a)(N()({minWidth:.535*window.screen.width}))&&(d="hidden",m="visible"),Object(C.jsxs)("div",{style:{width:"100%",height:window.innerHeight},children:[Object(C.jsxs)("div",{className:a.headBand,style:{width:window.innerWidth},children:[Object(C.jsx)(p.a,{className:a.title,children:"ad-datasets"}),Object(C.jsx)(p.a,{className:a.subTitle,children:"Complete* and curated list of autonomous driving related datasets"})]}),Object(C.jsxs)(T.a,{className:a.note,target:"_blank",style:{visibility:d},href:"https://github.com/daniel-bogdoll/ad-datasets",children:["*Could not find your dataset? ",Object(C.jsx)("br",{})," Simply create a pull request ;)"]}),Object(C.jsx)(T.a,{className:a.note,target:"_blank",style:{visibility:m},href:"https://github.com/daniel-bogdoll/ad-datasets",children:"*GitHub"}),Object(C.jsxs)(p.a,{className:a.lastUpdate,children:["Last Update: ","2022-06-01"]}),Object(C.jsxs)(p.a,{className:a.legend,children:[Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"row",alignItems:"flex-start",marginLeft:"2%"},children:[Object(C.jsx)(O,{}),Object(C.jsxs)("p",{children:["Number of Completely Analyzed Datasets: ",V," "]})]}),Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"row",alignItems:"flex-start",marginLeft:"2%"},children:[Object(C.jsx)(z,{}),Object(C.jsxs)("p",{children:["Number of Partially Analyzed Datasets: ",G," "]})]}),Object(C.jsxs)("div",{style:{display:"flex",flexDirection:"row",alignItems:"flex-start",marginLeft:"2%"},children:[Object(C.jsx)(A,{}),Object(C.jsxs)("p",{children:["Number of Datasets Missing Essential Information: ",M," "]})]})]}),Object(C.jsx)(l.a,{rows:U,columns:R,components:{Toolbar:l.b},disableColumnMenu:!0,sortingOrder:["desc","asc"],pageSize:o,onPageSizeChange:function(e){return c(e)},rowsPerPageOptions:[10,25,50],pagination:!0,disableSelectionOnClick:!0,columnBuffer:3,className:a.customDatagrid,style:{width:Window.innerWidth}})]})}var B=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,211)).then((function(t){var a=t.getCLS,i=t.getFID,n=t.getFCP,s=t.getLCP,o=t.getTTFB;a(e),i(e),n(e),s(e),o(e)}))};o.a.render(Object(C.jsx)(n.a.StrictMode,{children:Object(C.jsx)(F,{})}),document.getElementById("root")),B()},85:function(e){e.exports=JSON.parse('[{"id":"KITTI","href":"http://www.cvlibs.net/datasets/kitti/","size_hours":"6","size_storage":"180","frames":"-","numberOfScenes":"50","samplingRate":"10","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"2 greyscale cameras 1.4 MP, 2 color cameras 1.4 MP, 1 lidar 64 beams 360\xb0 10Hz, 1 inertial and GPS navigation system","benchmark":"stereo, optical flow, visual odometry, slam, 3d object detection, 3d object tracking","annotations":"3d bounding boxes","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 3.0","relatedDatasets":"Semantic KITTI, KITTI-360","publishDate":"2012-03-01","lastUpdate":"2021-02-01","relatedPaper":"http://www.cvlibs.net/publications/Geiger2013IJRR.pdf","location":"Karlsruhe, Germany","rawData":"Yes","DOI":"10.1177%2F0278364913491297","citationCount":4535,"completionStatus":"complete"},{"id":"Cars Dataset","href":"https://ai.stanford.edu/~jkrause/cars/car_dataset.html","size_storage":"-","size_hours":"-","frames":"16185","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"camera","recordingPerspective":"-","dataType":"Real","mapData":"No","benchmark":"-","annotations":"car make, model, year","licensing":"freely available for non-commercial research and educational use","relatedDatasets":"BMW-10","publishDate":"2013","lastUpdate":"-","paperTitle":"3D Object Representations for Fine-Grained Categorization","relatedPaper":"https://ai.stanford.edu/~jkrause/papers/3drr13.pdf","location":"Sourced from internet","rawData":"-","DOI":"10.1109/ICCVW.2013.77","citationCount":1529,"completionStatus":"complete"},{"id":"nuScenes","href":"https://www.nuscenes.org/","size_hours":"15","size_storage":"-","frames":"1400000","numberOfScenes":"1000","samplingRate":"-","lengthOfScenes":"20","sensors":"camera, lidar, radar, gps/imu","sensorDetail":"1x lidar 32 channels 360\xb0 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz","benchmark":"3d object detection, tracking, trajectory (prediction), lidar segmentation, panoptic segmentation & tracking","annotations":"semantic category, attributes, 3d bounding boxes ","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)","relatedDatasets":"nuImages","publishDate":"2019-03-01","lastUpdate":"2020-12-01","relatedPaper":"https://arxiv.org/pdf/1903.11027.pdf","location":"Boston, USA and Singapore","rawData":"Yes","DOI":"10.1109/cvpr42600.2020.01164","citationCount":1194,"completionStatus":"complete"},{"id":"nuImages","href":"https://www.nuscenes.org/nuimages","size_hours":"150","size_storage":"-","frames":"1200000","numberOfScenes":"93000","samplingRate":"2","lengthOfScenes":"-","sensors":"camera, lidar, radar, gps/imu","sensorDetail":"1x lidar 32 channels 360\xb0 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz","benchmark":"-","annotations":"instance masks, 2d bounding boxes, semantic segmentation masks, attribute annotations","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)","relatedDatasets":"nuScenes","publishDate":"2020-07-01","lastUpdate":"-","location":"Boston, USA and Singapore","rawData":"Yes","DOI":"10.1109/cvpr42600.2020.01164","citationCount":1194,"completionStatus":"partially Complete"},{"id":"GTA5","href":"https://download.visinf.tu-darmstadt.de/data/from_games/","size_storage":"57.05","size_hours":"-","frames":"24966","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"camera 1914x1052","recordingPerspective":"ego-perspective","dataType":"Synthetic","mapData":"No","benchmark":"-","annotations":"semantic segmentation of different classes","licensing":"Freely available for research and educational purposes","relatedDatasets":"-","publishDate":"2016-04-08","lastUpdate":"-","paperTitle":"Playing for Data: Ground Truth from Computer Games","relatedPaper":"https://arxiv.org/pdf/1608.02192.pdf","location":"Game: Grand Theft Auto 5","rawData":"-","DOI":"10.1007/978-3-319-46475-6_7","citationCount":1102,"completionStatus":"complete"},{"id":"Oxford Robot Car","href":"https://robotcar-dataset.robots.ox.ac.uk/","size_hours":"210","size_storage":"23150","frames":"-","numberOfScenes":"100","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, ins/gps","sensorDetail":"1x camera Bumblebee XB3 1280x960x3 16Hz, 3x camera Grasshopper2 1024x1024 12Hz, 2x lidar SICK LMS-151 270\xb0 50Hz, 1x lidar SICK LD-MRS 90\xb0 4 plane 12.5Hz, 1x NovAtel SPAN-CPT ALIGN 50Hz GPS+INS","benchmark":"-","annotations":"-","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International","relatedDatasets":"Oxford Radar Robot Car","publishDate":"2016-11-01","lastUpdate":"2020-02-01","relatedPaper":"https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf","location":"Oxford, UK","rawData":"Yes","DOI":"10.1177%2F0278364916679498","citationCount":808,"completionStatus":"complete"},{"id":"Caltech Pedestrian","href":"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/","size_storage":"-","size_hours":"10","frames":"1000000","numberOfScenes":"137","samplingRate":"30","lengthOfScenes":"60","sensors":"camera","sensorDetail":"1x camera 640x480 30Hz","benchmark":"pedestrian detection","annotations":"bounding boxes","licensing":"-","relatedDatasets":"-","publishDate":"2010-03-01","lastUpdate":"2019-01-01","relatedPaper":"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/files/CVPR09pedestrians.pdf","location":"Loa Angeles, USA","rawData":"Yes","DOI":"10.1109/CVPR.2009.5206631","citationCount":632,"completionStatus":"complete"},{"id":"Beyond PASCAL","href":"https://yuxng.github.io/Xiang_WACV_03242014.pdf","size_storage":"8.5","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","recordingPerspective":"-","dataType":"Real","mapData":"No","sensorDetail":"-","benchmark":"anomaly detection, 3D object detection and pose estimation","annotations":"label landmarks of the CAD model on the 2D image","licensing":"-","relatedDatasets":"-","publishDate":"2014-03-26","lastUpdate":"-","paperTitle":"Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild","relatedPaper":"https://cvgl.stanford.edu/papers/xiang_wacv14.pdf","location":"-","rawData":"-","DOI":"10.1109/WACV.2014.6836101","citationCount":607,"completionStatus":"complete"},{"id":"Waymo Open Perception","href":"https://waymo.com/open/data/perception/","size_hours":"10.83","size_storage":"-","frames":"390000","numberOfScenes":"1950","samplingRate":"10","lengthOfScenes":"20","sensors":"camera, lidar","sensorDetail":"5x cameras (front and sides) 1920x1280 & 1920x1040, 1x mid-range lidar, 4x short-range lidars","benchmark":"2d detection, 3d detection, 2d tracking, 3d tracking","annotations":"3d bounding boxes (lidar), 2d bounding boxes (camera)","licensing":"freely available for non-commercial purposes","relatedDatasets":"Waymo Open Motion","publishDate":"2019-08-01","lastUpdate":"2020-03-01","relatedPaper":"https://arxiv.org/pdf/1912.04838.pdf","location":"San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA","rawData":"Yes","DOI":"10.1109/CVPR42600.2020.00252","citationCount":547,"completionStatus":"complete"},{"id":"BDD100k","href":"https://www.bdd100k.com/","size_storage":"1800","size_hours":"1111","frames":"120000000","numberOfScenes":"100000","samplingRate":"30","lengthOfScenes":"40","sensors":"camera, gps/imu","sensorDetail":"crowd-sourced therefore no fixed setup, camera (720p) and gps/imu","benchmark":"object detection, instance segmentation, multiple object tracking, segmentation tracking, semantic segmentation, lane marking, drivable area, image tagging, imitation learning, domain adaption","annotations":"bounding boxes, instance segmentation, semantic segmentation, box tracking, semantic tracking, drivable area","licensing":"BSD 3-Clause","relatedDatasets":"-","publishDate":"2020-04-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1805.04687.pdf","location":"New York, Berkeley, San Francisco and Bay Area, USA","rawData":"Yes","citationCount":442,"completionStatus":"complete"},{"id":"Argoverse Motion Forecasting","href":"https://www.argoverse.org/","size_storage":"4.81","size_hours":"320","frames":"16227850","numberOfScenes":"324557","samplingRate":"10","lengthOfScenes":"5","sensors":"camera, lidar, gps","sensorDetail":"2x lidar 32 beam 40\xb0 10Hz, 7x ring cameras 1920x1200 combined 360\xb0 30Hz, 2x front-view facing stereo cameras 0.2986m baseline 2056x2464 5Hz","benchmark":"forecasting","annotations":"semantic vector map, rasterized map, trajectories","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)","relatedDatasets":"Argoverse 3D Tracking","publishDate":"2019-06-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1911.02620.pdf","location":"Miami and Pittsburgh, USA","rawData":"No","DOI":"10.1109/CVPR.2019.00895","citationCount":429,"completionStatus":"complete"},{"id":"Argoverse 3D Tracking","href":"https://www.argoverse.org/","size_storage":"254.4","size_hours":"1","frames":"44000","numberOfScenes":"113","samplingRate":"30","lengthOfScenes":"-","sensors":"camera, lidar, gps","sensorDetail":"2x lidar 40\xb0 10Hz, 7x ring cameras 1920x1200 combined 360\xb0 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz","benchmark":"tracking","annotations":"semantic vector map, rasterized map, 3d bounding boxes","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)","relatedDatasets":"Argoverse Motion Forecasting","publishDate":"2019-06-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1911.02620.pdf","location":"Miami and Pittsburgh, USA","rawData":"Yes","DOI":"10.1109/CVPR.2019.00895","citationCount":429,"completionStatus":"complete"},{"id":"Road Damage","href":"https://github.com/sekilab/RoadDamageDetector/","relatedPaper":"https://arxiv.org/pdf/1801.09454.pdf","citationCount":230,"completionStatus":"partially Complete"},{"id":"ApolloScape","href":"http://apolloscape.auto/","size_hours":"100","size_storage":"-","frames":"143906","numberOfScenes":"-","samplingRate":"30","lengthOfScenes":"-","sensors":"camera, lidar, imu/gnss","sensorDetail":"2x VUX-1HA laser scanners 360\xb0, 1x VMX-CS6 camera system, 1x measuring head with gnss/imu, 2x high frontal cameras 3384 \xd72710","benchmark":"2d image parsing, 3d car instance understanding, landmark segmentation, self-localization, trajectory prediction, 3d detection, 3d tracking, stereo","annotations":"high density 3d point cloud map, per-pixel, per-frame semantic image label, lane mark label semantic instance segmentation, geo-tagged","licensing":"freely available for non-commercial purposes","relatedDatasets":"-","publishDate":"2018-03-01","lastUpdate":"2020-09-01","relatedPaper":"https://arxiv.org/pdf/1803.06184.pdf","location":"Beijing, Shanghai and Shenzhen, China","rawData":"Yes","citationCount":216,"completionStatus":"complete"},{"id":"Brain4Cars","href":"http://brain4cars.com/","size_storage":"16","size_hours":"-","frames":"2000000","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, speed logger, gps","sensorDetail":"-","recordingPerspective":"Face camera, ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"driving events (lane changes, turns, driving straight)","licensing":"free for educational and non-commercial purposes","relatedDatasets":"-","publishDate":"2015-12-13","lastUpdate":"-","paperTitle":"Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models","relatedPaper":"http://brain4cars.com/pdfs/iccv2015.pdf","location":"USA","rawData":"-","DOI":"10.1109/ICCV.2015.364","citationCount":180,"completionStatus":"complete"},{"id":"comma.ai","href":"http://research.comma.ai/","size_storage":"80","size_hours":"7.25","frames":"-","numberOfScenes":"10","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"-","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"-","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License","relatedDatasets":"-","publishDate":"2016-08-03","lastUpdate":"-","paperTitle":"Learning a Driving Simulator","relatedPaper":"https://arxiv.org/pdf/1608.01230","location":"-","rawData":"-","DOI":"10.48550/arXiv.1608.01230","citationCount":176,"completionStatus":"complete"},{"id":"DDAD","href":"https://github.com/AdrienGaidon-TRI/DDAD","size_storage":"254","size_hours":"-","frames":"21200","numberOfScenes":"435","samplingRate":"10","lengthOfScenes":"-","sensors":"camera, lidar","sensorDetail":"6x cameras 2.4MP 1936x1216 10Hz, 1x Luminar-H2 Lidar sensor 360\xb0 10Hz","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"-","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License","relatedDatasets":"-","publishDate":"2020-06-19","lastUpdate":"-","paperTitle":"3D Packing for Self-Supervised Monocular Depth Estimation","relatedPaper":"https://arxiv.org/pdf/1905.02693.pdf","location":"USA (Ann Arbor, San Francisco Bay Area, Detroit, Cambridge and Massachusetts), Japan (Tokyo and Odaiba)","rawData":"-","DOI":"10.1109/CVPR42600.2020.00256","citationCount":175,"completionStatus":"complete"},{"id":"CARLA100","href":"https://github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md","relatedPaper":"https://arxiv.org/pdf/1904.08980.pdf","citationCount":153,"completionStatus":"partially Complete"},{"id":"Bosch Small Traffic Lights Dataset","href":"https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset","size_storage":"-","size_hours":"-","frames":"13427","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"raw 12bit HDR images with a red-clear-clear-blue filter 1280x720 & reconstructed 8-bit RGB color images 1280x720","benchmark":"-","annotations":"bounding boxes, state","licensing":"freely available for non-commercial purposes","relatedDatasets":"-","publishDate":"2017-05-01","lastUpdate":"-","relatedPaper":"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989163","location":"-","rawData":"Yes","DOI":"10.1109/ICRA.2017.7989163","citationCount":136,"completionStatus":"complete"},{"id":"Bosch TL","href":"https://github.com/asimonov/Bosch-TL-Dataset","size_storage":"-","size_hours":"-","frames":"13427","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"camera 1280x720","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"2d bounding boxes","licensing":"freely available for non-commercial use","relatedDatasets":"-","publishDate":"2017-07-24","lastUpdate":"-","paperTitle":"A deep learning approach to traffic lights: Detection, tracking, and classification","relatedPaper":"https://ieeexplore.ieee.org/document/7989163","location":"El Camino Real in the San Francisco Bay Area, California","rawData":"-","DOI":"10.1109/ICRA.2017.7989163","citationCount":136,"completionStatus":"complete"},{"id":"Oxford Radar Robot Car","href":"https://oxford-robotics-institute.github.io/radar-robotcar-dataset/","size_storage":"4700","size_hours":"-","frames":"-","numberOfScenes":"32","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, radar, gps/imu","sensorDetail":"1 x Navtech CTS350-X Millimetre-Wave FMCW radar 4 Hz, 2 x Velodyne HDL-32E LIDAR 360\xb032 planes 20 Hz, 1 x Point Grey Bumblebee XB3 trinocular stereo camera 1280\xd7960\xd73 16 Hz 66\xb03 x Point Grey Grasshopper2 1024\xd71024 11.1 Hz 180\xb0, 2 x SICK LMS-151 2D LIDAR 270\xb0 50Hz, 1 x NovAtel SPAN-CPT ALIGN inertial and GPS navigation system 6 axis 50Hz,","benchmark":"-","annotations":"ground truth data","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International","relatedDatasets":"Oxford Robot Car","publishDate":"2020-02-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1909.01300.pdf","location":"Oxford","rawData":"Yes","citationCount":129,"completionStatus":"complete"},{"id":"INTERACTION dataset","href":"https://interaction-dataset.com/","size_storage":"-","size_hours":"16.5","frames":"594588","numberOfScenes":"-","samplingRate":"10","lengthOfScenes":"-","sensors":"camera","sensorDetail":"drones & traffic cameras 3840x2160 30Hz downscaled to 10Hz","benchmark":"motion prediction","annotations":"2d bounding boxes, semantic map, motion/trajectories","licensing":"freely available for non-commercial purposes","relatedDatasets":"-","publishDate":"2019-09-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1910.03088.pdf","location":"USA, China, Germany and Bulgaria","rawData":"Yes","citationCount":128,"completionStatus":"complete"},{"id":"India Driving Dataset","href":"https://idd.insaan.iiit.ac.in/","size_storage":"-","size_hours":"-","frames":"10004","numberOfScenes":"182","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"1080p & 720p stereo image","benchmark":"Pixel-Level Semantic Segmentation Task, Instance-Level Semantic Segmentation Task","annotations":"semantic segmentation","licensing":"-","relatedDatasets":"-","publishDate":"2018-11-01","lastUpdate":"-","relatedPaper":"https://idd.insaan.iiit.ac.in/media/publications/idd-650.pdf","location":"Bangalore and Hyderabad, India","rawData":"Yes","DOI":"10.1109/WACV.2019.00190","citationCount":123,"completionStatus":"complete"},{"id":"KITTI-360","href":"http://www.cvlibs.net/datasets/kitti-360/","size_storage":"-","size_hours":"-","frames":"400000","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"2x 180\xb0 fisheye camera, 1x 90\xb0 perspective stereo camera, 1x Velodyne HDL-64E & SICK LMS 200 laser scanning unit in pushbroom configuration","benchmark":"-","annotations":"semantic instance segmentation","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 3.0","relatedDatasets":"KITTI","publishDate":"2015-11-01","lastUpdate":"2021-04-01","relatedPaper":"https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.pdf","location":"Karlsruhe, Germany","rawData":"Yes","DOI":"10.1109/CVPR.2016.401","citationCount":121,"completionStatus":"complete"},{"id":"KAIST Multi-Spectral Day/Night","href":"http://multispectral.kaist.ac.kr","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"25","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu, thermal camera","sensorDetail":"2x PointGrey Flea3 RGB camera 1280 \xd7 960, 1x FLIR A655Sc thermal camera 640x480 50Hz, 1x Velodyne HDL-32E 3D LiDAR 360\xb0 32 beams 10Hz, 1x OXTS RT2002 gps/ins 100Hz","benchmark":"object detection, vision sensor enhancement, depth estimation, multi-spectral colorization","annotations":"dense depth map, bounding boxes","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 3.0","relatedDatasets":"-","publishDate":"2017-12","lastUpdate":"-","relatedPaper":"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8293689","location":"-","rawData":"Yes","DOI":"10.1109/TITS.2018.2791533","citationCount":106,"completionStatus":"complete"},{"id":"JAAD","href":"https://paperswithcode.com/dataset/jaad","size_storage":"3.1","size_hours":"-","frames":"82032","numberOfScenes":"346","samplingRate":"30","lengthOfScenes":"-","sensors":"camera","sensorDetail":"1x camera 1920x1080 110\xb0, 1x camera 1920x1080 170\xb0, 1x camera 1280x720 100\xb0","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"2d bounding boxes, behavioral tags, contextual tags","licensing":"MIT License","relatedDatasets":"PIE Dataset","publishDate":"2016-09-15","lastUpdate":"-","paperTitle":"Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior","relatedPaper":"https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf","location":"North America and Europe","rawData":"-","DOI":"10.1109/ICCVW.2017.33","citationCount":106,"completionStatus":"complete"},{"id":"H3D","href":"https://paperswithcode.com/dataset/h3d","size_storage":"-","size_hours":"0.77","frames":"27721","numberOfScenes":"160","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"3x color PointGrey Grasshopper3 video cameras 1920x1200 90\xb0/80\xb0 30Hz, 1x Velodyne HDL-64E LiDAR 64 beams 360\xb0 10Hz, 1x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer 100Hz","benchmark":"-","annotations":"3d bounding boxes","licensing":"-","relatedDatasets":"-","publishDate":"2019-03-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1903.01568.pdf","location":"San Francisco Bay Area, USA","rawData":"Yes","citationCount":97,"completionStatus":"complete"},{"id":"KAIST Urban","href":"https://irap.kaist.ac.kr/dataset/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"18","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"2x Velodyne VLP-16 16 channel lidar 360\xb0 10Hz, 2x SICK LMS-511 1 channel lidar 190\xb0 100Hz, 1x stereo camera 1280x560 10Hz","benchmark":"-","annotations":"-","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0","relatedDatasets":"-","publishDate":"2017-09-01","lastUpdate":"2019-06-01","relatedPaper":"https://irap.kaist.ac.kr/dataset/papers/IJRR2019_dataset.pdf","location":"Seoul, Pangyo, Daejeon, Suwon and Dongtan, Korea","rawData":"Yes","DOI":"10.1177%2F0278364919843996","citationCount":89,"completionStatus":"complete"},{"id":"Synscapes","href":"https://7dlabs.com/synscapes-overview","size_storage":"-","size_hours":"-","frames":"25000","numberOfScenes":"25000","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"RGB images in PNG format 1440x720 & upscaled version 2048x1024","benchmark":"-","annotations":"2d bounding boxes, 3d bounding boxes, occlusion, truncation, semantic segmentation,instance segmentation, depth segmentation, scene metadata","licensing":"freely available for non-commercial purposes","relatedDatasets":"-","publishDate":"2018-10-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1810.08705v1.pdf","location":"-","rawData":"-","citationCount":82,"completionStatus":"complete"},{"id":"WildDash","href":"https://wilddash.cc/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"156","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"various sources, e.g. YouTube","benchmark":"semantic segmentation, instance segmentation, panoptic segmentation","annotations":"semantic segmentation, instance segmentation","licensing":"CC-BY-NC 4.0 ","relatedDatasets":"-","publishDate":"2018-02-01","lastUpdate":"2020-06-01","relatedPaper":"https://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf","location":"All over the world","rawData":"Yes","DOI":"10.1007/978-3-030-01231-1_25","citationCount":81,"completionStatus":"complete"},{"id":"Lyft Level5 Prediction","href":"https://level-5.global/data/prediction/","size_hours":"1118","size_storage":"-","frames":"42500000","numberOfScenes":"170000","samplingRate":"10","lengthOfScenes":"25","sensors":"camera, lidar, radar","sensorDetail":"7 cameras with 360\xb0 view, 3 lidars with 40-64 channels at 10Hz, 5 radars","benchmark":"-","annotations":"semantic map \\"annotations\\", trajectories","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)","relatedDatasets":"Lyft Level5 Perception","publishDate":"2020-06-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2006.14480v1.pdf","location":"Palo Alto, USA","rawData":"No","citationCount":75,"completionStatus":"complete"},{"id":"Lyft Level5 Perception","href":"https://level-5.global/data/perception/","size_hours":"2.5","size_storage":"-","frames":"-","numberOfScenes":"366","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar","sensorDetail":"-","benchmark":"-","annotations":"3d bounding boxes, rasterised road geometry","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)","relatedDatasets":"Lyft Level5 Prediction","publishDate":"2019-07-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2006.14480v1.pdf","location":"Palo Alto, USA","rawData":"Yes","citationCount":75,"completionStatus":"complete"},{"id":"A2D2","href":"https://www.a2d2.audi/a2d2/en.html","size_storage":"2300","size_hours":"-","frames":"433833","numberOfScenes":"3","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"5x lidar 16 channels 360\xb0 10Hz, 1x front centre camera 1920x1208 30Hz, 5x surround cameras1920x1208 30Hz, vehicle bus data","benchmark":"-","annotations":"semantic segmentation, point cloud segmentation, instance segmentation, 3d bounding boxes","licensing":"Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)","relatedDatasets":"-","publishDate":"2020-04-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2004.06320.pdf","location":"Three cities in the south of Germany","rawData":"Yes","citationCount":71,"completionStatus":"complete"},{"id":"LostAndFound","href":"http://www.6d-vision.com/lostandfounddataset","size_storage":"-","size_hours":"-","frames":"21040","numberOfScenes":"112","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"stereo camera setup baseline 21cm 2048x1024","benchmark":"anomaly detection","annotations":"semantic segmentation","licensing":"freely available for non-commercial purposes","relatedDatasets":"-","publishDate":"2016-09-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1609.04653.pdf","location":"-","rawData":"Yes","citationCount":62,"completionStatus":"complete"},{"id":"SeeingThroughFog","href":"https://github.com/princeton-computational-imaging/SeeingThroughFog","size_storage":"-","size_hours":"-","frames":"1400000","numberOfScenes":"-","samplingRate":"10","lengthOfScenes":"-","sensors":"camera, radar, lidar, imu, weather sensor","sensorDetail":"2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)","recordingPerspective":"Ego-perspective","dataType":"real","mapData":"No","benchmark":"-","annotations":"weather of scenes in frames","licensing":"Freely available for research and teaching purposes","relatedDatasets":"Gated2Gated, Gated2Depth, PointCloudDeNoising","publishDate":"2019-02-24","lastUpdate":"-","paperTitle":"Seeing Through FogWithout Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather","relatedPaper":"https://www.cs.princeton.edu/~fheide/AdverseWeatherFusion/figures/AdverseWeatherFusion.pdf","location":"Germany, Sweden, Denmark and Finland","rawData":"-","DOI":"10.1109/CVPR42600.2020.01170","citationCount":57,"completionStatus":"complete"},{"id":"Fishyscapes","href":"https://fishyscapes.com/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"based on the validation set of Cityscapes overlayed with anomalous objects and the original LostAndFound with extended pixel-wise annotations","benchmark":"anomaly detection, semantic segmentation","annotations":"semantic segmentation","licensing":"-","relatedDatasets":"Cityscapes, LostAndFound","publishDate":"2019-09-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1904.03215.pdf","location":"-","rawData":"No","citationCount":45,"completionStatus":"complete"},{"id":"CADC","href":"http://cadcd.uwaterloo.ca/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"75","samplingRate":"-","lengthOfScenes":"-","sensors":"cameras, lidar, gps","sensorDetail":"8x camera Ximea MQ013CG-E2 1280x1024 10Hz,  1x lidar Veldyne VLP-32C 360\xb0 10Hz, 1x NovAtel OEM638 Triple-Frequency GPS, 1x Sensonor STIM300 MEMS 100Hz IMU, 2x Xsens 200Hz IMU","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"Yes","benchmark":"-","annotations":"2d/3d bounding boxes","licensing":"Creative Commons Attribution-NonCommercial 4.0 International Public License (CC BY-NC 4.0)","relatedDatasets":"-","publishDate":"2020-01-27","lastUpdate":"-","paperTitle":"Canadian Adverse Driving Conditions Dataset","relatedPaper":"https://arxiv.org/pdf/2001.10117.pdf","location":"Waterloo region in Ontorio, Canada","rawData":"-","DOI":"10.1177/0278364920979368","citationCount":44,"completionStatus":"complete"},{"id":"NightOwls","href":"https://www.nightowls-dataset.org/","size_storage":"-","size_hours":"5.17","frames":"279000","numberOfScenes":"40","samplingRate":"15","lengthOfScenes":"-","sensors":"camera","sensorDetail":"1x industry standard 1024x640 camera","benchmark":"pedestrian detection, object detection","annotations":"bounding boxes, attributes, temporal tracking annotations","licensing":"freely available for non-commercial purposes","relatedDatasets":"-","publishDate":"2018-12-01","lastUpdate":"-","relatedPaper":"https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/neumann18b.pdf","location":"Several cities across Europe","rawData":"Yes","DOI":"10.1007/978-3-030-20887-5_43","citationCount":40,"completionStatus":"complete"},{"id":"SemKITTI-DVPS","href":"https://github.com/joe-siyuan-qiao/ViP-DeepLab","relatedDatasets":"SemanticKITTI","relatedPaper":"https://arxiv.org/pdf/2012.05258","location":"Karlsruhe, Germany","rawData":"Yes","citationCount":40,"completionStatus":"partially Complete"},{"id":"Cityscapes-DVPS","href":"https://github.com/joe-siyuan-qiao/ViP-DeepLab","relatedDatasets":"Cityscapes","relatedPaper":"https://arxiv.org/pdf/2012.05258","rawData":"Yes","citationCount":40,"completionStatus":"partially Complete"},{"id":"Waymo Open Motion","href":"https://waymo.com/open/data/motion/","size_hours":"574","size_storage":"-","frames":"20670800","numberOfScenes":"103354","samplingRate":"10","lengthOfScenes":"20","sensors":"camera, lidar","sensorDetail":"5x cameras, 5x lidar, ","benchmark":"motion prediction, interaction prediction","annotations":"3d bounding boxes, 3d hd map information","licensing":"freely available for non-commercial purposes","relatedDatasets":"Waymo Open Perception","publishDate":"2021-03-01","lastUpdate":"2021-09-01","relatedPaper":"https://arxiv.org/pdf/2104.10133.pdf","location":"San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA","rawData":"No","citationCount":37,"completionStatus":"complete"},{"id":"The EuroCity Persons Dataset","href":"https://arxiv.org/abs/1805.07193","size_storage":"-","size_hours":"-","frames":"47300","numberOfScenes":"-","samplingRate":"20","lengthOfScenes":"-","sensors":"camera","sensorDetail":"1x camera 1920x1080 20Hz","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"Yes","benchmark":"object detection","annotations":"2d bounding boxes","licensing":"freely available for research use by eligiible persons","relatedDatasets":"ECP2.5D - Person Localization in Traffic Scenes","publishDate":"2019-03-31","lastUpdate":"2020-11-11","paperTitle":"The EuroCity Persons Dataset: A Novel Benchmark for Object Detection","relatedPaper":"https://arxiv.org/pdf/1805.07193.pdf","location":"12 countries and 31 cities across Europe","rawData":"-","DOI":"10.1109/TPAMI.2019.2897684","citationCount":33,"completionStatus":"complete"},{"id":"D^2 City","href":"https://outreach.didichuxing.com/d2city","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"11211","samplingRate":"25","lengthOfScenes":"30","sensors":"camera","sensorDetail":"","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"2d bounding boxes and inter-frame tracking labels","licensing":"-","relatedDatasets":"-","publishDate":"2019-04-03","lastUpdate":"-","paperTitle":"D^2-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios","relatedPaper":"https://arxiv.org/pdf/1904.01975v1.pdf","location":"5 Chinese cities","rawData":"-","DOI":"10.48550/arXiv.1904.01975","citationCount":30,"completionStatus":"complete"},{"id":"Comma2k19","href":"https://github.com/commaai/comma2k19","size_storage":"100","size_hours":"33.65","frames":"-","numberOfScenes":"2019","samplingRate":"-","lengthOfScenes":"60","sensors":"camera, radar, gnss/imu ","sensorDetail":"two different car types, 1x road-facing camera Sony IMX2984 20Hz, 1x gnss u-blox M8 chip5 10Hz, gyro and accelerometer data LSM6DS3 100Hz, magnetometer data AK09911 10Hz","benchmark":"-","annotations":"-","licensing":"MIT","relatedDatasets":"-","publishDate":"2018-12-01","lastUpdate":"-","relatedPaper":"http://export.arxiv.org/pdf/1812.05752","location":"California\'s 280 highway, USA","rawData":"Yes","citationCount":28,"completionStatus":"complete"},{"id":"RUGD: Robot Unstructured Ground Driving","href":"http://rugd.vision/","size_storage":"5.4","size_hours":"-","frames":"37000","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"1x Prosilica GT2750C camera 1376x1110, 1x Velodyne HDL-32 LiDAR, 1x Garmin GPS receiver, 1x Microstrain GX3-25 IMU","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"semantic segmentation","annotations":"","licensing":"Freely available for research purposes","relatedDatasets":"-","publishDate":"2019-11-08","lastUpdate":"-","paperTitle":"A RUGD Dataset for Autonomous Navigation and Visual Perception in Unstructured Outdoor Environments","relatedPaper":"http://rugd.vision/pdfs/RUGD_IROS2019.pdf","location":"-","rawData":"-","DOI":"10.1109/IROS40897.2019.8968283","citationCount":28,"completionStatus":"complete"},{"id":"UTBM EU LTD","href":"https://epan-utbm.github.io/utbm_robocar_dataset/","relatedPaper":"https://arxiv.org/pdf/1909.03330.pdf","rawData":"Yes","publishDate":"2020-08-01","citationCount":26,"completionStatus":"partially Complete"},{"id":"RADIATE","href":"http://pro.hw.ac.uk/radiate/","size_storage":"-","size_hours":"5","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, radar, gps/imu","sensorDetail":"1x ZED stereo camera 672x376 15Hz, 1x Velodyne HDL-32e LiDAR 32 channel 360\xb0 10Hz, 1x Navtech CTS350-X radar 360\xb0, 1x Advanced Navigation Spatial Dual GPS/IMU","benchmark":"-","annotations":"2d bounding boxes","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International","relatedDatasets":"-","publishDate":"2020-10-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2010.09076.pdf","location":"-","rawData":"Yes","citationCount":25,"completionStatus":"complete"},{"id":"Talk2Car","href":"https://talk2car.github.io/","size_storage":"300","size_hours":"-","frames":"-","numberOfScenes":"850","samplingRate":"-","lengthOfScenes":"20","sensors":"camera, lidar, radar, gps/imu","sensorDetail":"1x lidar 32 channels 360\xb0 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz","recordingPerspective":"Ego-perspective","dataType":"Real","mapData":"No","benchmark":"find bounding box for objects based on user commands in natural language","annotations":"bounding box and command in natural language related to the bounding box","licensing":"MIT license","relatedDatasets":"-","publishDate":"2020-03-18","lastUpdate":"2021-10-06","paperTitle":"Talk2Car: Taking Control of Your Self-Driving Car","relatedPaper":"https://arxiv.org/pdf/1909.10838.pdf","location":"Boston and Singapore","rawData":"-","DOI":"10.18653/v1/D19-1215","citationCount":25,"completionStatus":"complete"},{"id":"Ford Autonomous Vehicle Dataset","href":"https://avdata.ford.com/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"4x HDL-32E Lidars, 4x Flea3 GigE Point Grey Cameras in stereo pairs (front & back) 80\xb0 15Hz,2x Flea3 GigE Point Grey Cameras (sides) 80\xb0 15Hz, 1x Flea3 GigE Point Grey Camera 40\xb0 7Hz, 1x Applanix POS LV gps/imu","benchmark":"-","annotations":"3d point cloud maps, ground reflectivity map","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International","relatedDatasets":"-","publishDate":"2020-03-01","lastUpdate":"-","relatedPaper":"https://s23.q4cdn.com/258866874/files/doc_downloads/2020/03/2003.07969.pdf","location":"Michigan, USA","rawData":"True","DOI":"10.1177/0278364920961451","citationCount":24,"completionStatus":"complete"},{"id":"CADP","href":"https://ankitshah009.github.io/accident_forecasting_traffic_camera","size_storage":"-","size_hours":"5.2","frames":"-","numberOfScenes":"1416","samplingRate":"20","lengthOfScenes":"-","sensors":"camera","sensorDetail":"-","recordingPerspective":"Bird\'s Eye","dataType":"Real","mapData":"No","benchmark":"-","annotations":"Object detectiona and accident forecasting","licensing":"freely available for non-commercial use","relatedDatasets":"-","publishDate":"2018-10-04","lastUpdate":"-","paperTitle":"CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis","relatedPaper":"https://ppms.cit.cmu.edu/media/project_files/CADP_IEEE_Camera_Ready_Final.pdf","location":"Videos sampled from YouTube","rawData":"-","DOI":"10.1109/AVSS.2018.8639160","citationCount":24,"completionStatus":"complete"},{"id":"DDD 20","href":"https://sites.google.com/view/davis-driving-dataset-2020/home","size_storage":"1300","size_hours":"51","frames":"-","numberOfScenes":"216","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, car parameters","sensorDetail":"1x DAVIS346B 346x260 up to 50Hz, vehicle bus data","benchmark":"-","annotations":"-","licensing":"Creative Commons Attribution-ShareAlike 4.0 International","relatedDatasets":"DDD 17","publishDate":"2020-02-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2005.08605.pdf","location":"California, USA","rawData":"Yes","citationCount":23,"completionStatus":"complete"},{"id":"RELLIS-3D Dataset","href":"https://unmannedlab.github.io/research/RELLIS-3D","size_storage":"58.1","size_hours":"-","frames":"13556","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"1x Nerian Karmin2 + Nerian SceneScan: 3D StereoCamera 10Hz, 1x RGB Camera: Basler acA1920-50gc camera with 16mm/F18 EDMUND Optics lens 1920x1200 10Hz, 1x Ouster OS1 LiDAR 64 Channels 10 Hz, 1x Velodyne Ultra Puck: 32 Channels 10Hz, Vectornav VN-300 Dual Antenna GNSS/INS 300Hz GPS, 100Hz IMU","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"semantic segmentation of 2d image and 3d point clouds","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License","relatedDatasets":"SemanticUSL: A Dataset for LiDAR Semantic Segmentation Domain Adaptation","publishDate":"2020-11-26","lastUpdate":"2022-01-24","paperTitle":"RELLIS-3D Dataset: Data, Benchmarks and Analysis","relatedPaper":"https://arxiv.org/pdf/2011.12954.pdf","location":"Rellis Campus of Texas A&M University","rawData":"-","DOI":"10.1109/ICRA48506.2021.9561251","citationCount":23,"completionStatus":"complete"},{"id":"Seasonal Variation Dataset","href":"http://www.cs.cmu.edu/~aayushb/localization/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, imu/gps","sensorDetail":"1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"Yes","benchmark":"-","annotations":"-","licensing":"Publicly available for research community","relatedDatasets":"Bay Area Dataset, Illumination Changes in a day","publishDate":"2014-06-11","lastUpdate":"-","paperTitle":"Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization","relatedPaper":"http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf","location":"California bay area and Pittsburgh, USA","rawData":"Yes","DOI":"10.1109/IVS.2014.6856605","citationCount":23,"completionStatus":"complete"},{"id":"Bay Area Dataset","href":"http://www.cs.cmu.edu/~aayushb/localization/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, imu/gps","sensorDetail":"1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"Yes","benchmark":"-","annotations":"-","licensing":"Available on request","relatedDatasets":"Seasonal Variation Dataset, Illumination Changes in a day","publishDate":"2014-06-11","lastUpdate":"-","paperTitle":"Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization","relatedPaper":"http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf","location":"California bay area and Pittsburgh, USA","rawData":"Yes","DOI":"10.1109/IVS.2014.6856605","citationCount":23,"completionStatus":"complete"},{"id":"Illumination Changes in a day","href":"http://www.cs.cmu.edu/~aayushb/localization/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, imu/gps","sensorDetail":"1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"Yes","benchmark":"-","annotations":"-","licensing":"Available on request","relatedDatasets":"Bay Area Dataset, Seasonal Variation Dataset","publishDate":"2014-06-11","lastUpdate":"-","paperTitle":"Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization","relatedPaper":"http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf","location":"California bay area and Pittsburgh, USA","rawData":"Yes","DOI":"10.1109/IVS.2014.6856605","citationCount":23,"completionStatus":"complete"},{"id":"PointCloudDeNoising","href":"https://github.com/rheinzler/PointCloudDeNoising","size_storage":"-","size_hours":"-","frames":"175941","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"lidar","sensorDetail":"1 Velodyne VLP32c lidar sensor","recordingPerspective":"Ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"pointwise annotations","licensing":"Freely available for research and teaching purposes","relatedDatasets":"Gated2Depth, Gated2Gated, SeeingThroughFog","publishDate":"2019-12-09","lastUpdate":"-","paperTitle":"CNN-based Lidar Point Cloud De-Noising in Adverse Weather","relatedPaper":"https://ieeexplore.ieee.org/document/8990038","location":"CEREMA\'s climatic chamber","rawData":"-","DOI":"10.1109/LRA.2020.2972865","citationCount":22,"completionStatus":"complete"},{"id":"ONCE","href":"https://once-for-auto-driving.github.io/index.html","size_storage":"-","size_hours":"144","frames":"1000000","numberOfScenes":"581","samplingRate":"10","lengthOfScenes":"-","sensors":"camera, lidar","sensorDetail":"7x color cameras 1920x1020 10Hz, 1x 40-beam lidar 360\xb0 10Hz","recordingPerspective":"ego-perspective","dataType":"Real","benchmark":"3d object detection","annotations":"2d/3d bounding boxes","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)","relatedDatasets":"ONCE-3DLanes","publishDate":"2021-05-18","lastUpdate":"2021-08-05","paperTitle":"One Million Scenes for Autonomous Driving: ONCE Dataset","relatedPaper":"https://arxiv.org/pdf/2106.11037.pdf","location":"China","rawData":"-","DOI":"10.48550/arXiv.2106.11037","citationCount":21,"completionStatus":"complete"},{"id":"Gated2Depth","href":"https://github.com/gruberto/Gated2Depth","size_storage":"-","size_hours":"-","frames":"17686","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar","sensorDetail":"1x Aptina AR0230 stereo camera 1920x1080 30Hz, 1x Velodyne HDL64-S3 lidar, 1x gated camera 10bit images 1280x720 120Hz","recordingPerspective":"Ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"-","licensing":"Freely available for research and teaching purposes","relatedDatasets":"SeeingThroughFog, PointCloudDeNoising, Gated2Gated","publishDate":"2020-02-13","lastUpdate":"2020-04-12","paperTitle":"Gated2Depth: Real-Time Dense Lidar From Gated Images","relatedPaper":"https://arxiv.org/pdf/1902.04997.pdf","location":"Germany, Denmark and Sweden","rawData":"-","DOI":"10.1109/ICCV.2019.00159","citationCount":21,"completionStatus":"complete"},{"id":"DriveU Traffic Light Dataset","href":"https://www.uni-ulm.de/en/in/driveu/projects/driveu-traffic-light-dataset/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"-","samplingRate":"15","lengthOfScenes":"-","sensors":"camera","sensorDetail":"1x stereo camera 60\xb0, 1x stereo camera 130\xb0","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"Yes","benchmark":"-","annotations":"2d bounding boxes","licensing":"freely available for non-commercial research and teaching activities","relatedDatasets":"-","publishDate":"2018-11-27","lastUpdate":"2021-04","paperTitle":"The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets","relatedPaper":"https://www.researchgate.net/profile/Julian-Mueller-14/publication/327808220_The_DriveU_Traffic_Light_Dataset_Introduction_and_Comparison_with_Existing_Datasets/links/5c1910e4a6fdccfc7056b787/The-DriveU-Traffic-Light-Dataset-Introduction-and-Comparison-with-Existing-Datasets.pdf ","location":"10 cities across Germany","rawData":"-","DOI":"10.1109/ICRA.2018.8460737","citationCount":20,"completionStatus":"complete"},{"id":"CARRADA Dataset","href":"https://github.com/valeoai/carrada_dataset","size_storage":"288","size_hours":"-","frames":"12666","numberOfScenes":"30","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, radar","sensorDetail":"1x camera 1238x1024,  1x radar 180\xb0","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"range-angle Doppler annotations","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)","relatedDatasets":"-","publishDate":"2020-05-04","lastUpdate":"2021-07","paperTitle":"CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations","relatedPaper":"https://arxiv.org/pdf/2005.01456.pdf","location":"-","rawData":"-","DOI":"10.1109/ICPR48806.2021.9413181","citationCount":19,"completionStatus":"complete"},{"id":"4Seasons","href":"https://www.4seasons-dataset.com/","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"30","samplingRate":"30","lengthOfScenes":"-","sensors":"camera, imu/rtk-gnss","sensorDetail":"2x cameras stereo baseline 30cm 800x400 (after cropping)","benchmark":"globally consistent reference poses","annotations":"-","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)","relatedDatasets":"-","publishDate":"2020-10-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2009.06364.pdf","location":"-","rawData":"Yes","citationCount":16,"completionStatus":"complete"},{"id":"IDDA","href":"https://idda-dataset.github.io/home/","relatedPaper":"https://arxiv.org/pdf/2004.08298.pdf","benchmark":"semantic segmentation","rawData":"Yes","citationCount":14,"completionStatus":"partially Complete"},{"id":"RoadAnomaly21","href":"https://segmentmeifyoucan.com/datasets","size_storage":"0.05","size_hours":"-","frames":"100","numberOfScenes":"100","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"images from web resources 2048x1024 & 1280x720","benchmark":"anomaly detection","annotations":"semantic segmentation","licensing":"various, see \\"https://github.com/SegmentMeIfYouCan/road-anomaly-\\"benchmark\\"/blob/master/doc/RoadAnomaly/credits.txt\\" for detail","relatedDatasets":"RoadObstacle21","publishDate":"2021-04-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2104.14812.pdf","location":"-","rawData":"Yes","citationCount":13,"completionStatus":"complete"},{"id":"RadarScenes","href":"https://radar-scenes.com/","size_storage":"-","size_hours":"4","frames":"-","numberOfScenes":"158","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, radar, odometry","sensorDetail":"4x 77 GHz series production automotive 60\xb0 radar sensor, 1x documentary camera","benchmark":"-","annotations":"point-wise","licensing":"Creative Commons Attribution Non Commercial Share Alike 4.0 International","relatedDatasets":"-","publishDate":"2021-03-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2104.02493.pdf","location":"-","rawData":"Yes","citationCount":13,"completionStatus":"complete"},{"id":"Cityscapes 3D","href":"https://www.cityscapes-dataset.com/","size_hours":"-","size_storage":"63.141","frames":"-","numberOfScenes":"-","samplingRate":"17","lengthOfScenes":"1.8","sensors":"camera, gps, thermometer","sensorDetail":"stereo cameras 22 cm baseline 17Hz, odometry from in-vehicle \\"sensors\\" & outs\\"id\\"e temperature & GPS tracks","benchmark":"pixel-level semantic labeling, instance-level semantic labeling, panoptic semantic sabeling 3d vehicle detection","annotations":"dense semantic segmentation, instance segmentation for vehicles & people, 3d bounding boxes","licensing":"freely available for non-commercial purposes","relatedDatasets":"-","publishDate":"2016-02-01","lastUpdate":"2020-10-01","relatedPaper":"https://arxiv.org/pdf/2006.07864.pdf","location":"50 cities in Germany and neighboring countries","rawData":"Yes","citationCount":12,"completionStatus":"complete"},{"id":"MOTSynth","href":"https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=42","relatedPaper":"https://arxiv.org/pdf/2108.09518.pdf","citationCount":10,"completionStatus":"partially Complete"},{"id":"Semantic KITTI","href":"http://www.semantic-kitti.org/","size_storage":"-","size_hours":"-","frames":"43552","numberOfScenes":"21","samplingRate":"10","lengthOfScenes":"-","sensors":"lidar","sensorDetail":"Velodyne HDL-64E from sequences of the odometry \'benchmark\' of the KITTI Vision Benchmark with 360\xb0 view","benchmark":"semantic segmentation, panoptic segmentation, 4D panoptic segmentation, moving object segmentation, semantic scene completion","annotations":"semantic segmentation","licensing":"Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) ","relatedDatasets":"KITTI","publishDate":"2019-07-01","lastUpdate":"2021-02-01","relatedPaper":"https://arxiv.org/abs/1904.01416.pdf","location":"Karlsruhe, Germany","rawData":"No","citationCount":8,"completionStatus":"complete"},{"id":"MCity Data Collection","href":"https://arxiv.org/pdf/1912.06258.pdf","size_storage":"11000","size_hours":"50","frames":"-","numberOfScenes":"255","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, radar, gps/imu","sensorDetail":"3x Velodyne Ultra Puck VLP-32C lidar 10Hz, 2x forward-facing cameras 30\xb0 1080P 30Hz,1x backward-facing camera 90\xb0 1080P 30Hz, 1x cabin pose camera 1280\xd71080 30Hz, 1x cabin head/eyeball camera 640P 30Hz, 1x Ibeo four beam LUX sensor 25Hz, 1x Delphi ESR 2.5 Radar 90\xb0 20Hz,1x NovAtel FlexPak6 with IMU-IGM-S1 and 4G cellular for RTK GPS single antenna 1Hz","benchmark":"-","annotations":"semantic segmentation of objects, traffic lights, traffic signs, lanes","licensing":"-","relatedDatasets":"-","publishDate":"2019-12-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/1912.06258.pdf","location":"Ann Arbor, USA","rawData":"Yes","citationCount":8,"completionStatus":"complete"},{"id":"openDD","href":"https://l3pilot.eu/data/opendd","size_storage":"-","size_hours":"62.7","frames":"6771600","numberOfScenes":"501","samplingRate":"30","lengthOfScenes":"-","sensors":"camera","sensorDetail":"DJI Phantom 4 3840\xd72160 camera drone","benchmark":"trajectory predictions","annotations":"2d bounding boxes, trajectories","licensing":"Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) ","relatedDatasets":"-","publishData":"2020-09-01","lastUpdate":"-","relatedPaper":"https://arxiv.org/pdf/2007.08463.pdf","location":"Wolfsburg and Ingolstadt, Germany","rawData":"Yes","citationCount":7,"completionStatus":"partially Complete"},{"id":"PepScenes","href":"https://github.com/huawei-noah/PePScenes","relatedPaper":"https://arxiv.org/pdf/2012.07773.pdf","citationCount":4,"completionStatus":"partially Complete"},{"id":"ROAD","href":"https://github.com/gurkirt/road-dataset","size_hours":"2.83","size_storage":"-","frames":"122000","numberOfScenes":"22","samplingRate":"12","lengthOfScenes":"-","sensors":"camera","sensorDetail":"","recordingPerspective":"ego-perspective","dataType":"Real","benchmark":"agent detection, action detection and road event detection","annotations":"2d/3d bounding boxes, action label and location labels","licensing":"Creative Commons Attribution Share Alike 4.0 International","relatedDatasets":"Oxford Robot Car Dataset (OxRD)","publishDate":"-","lastUpdate":"-","paperTitle":"ROAD: The ROad event Awareness Dataset for Autonomous Driving","relatedPaper":"https://www.computer.org/csdl/api/v1/periodical/trans/tp/5555/01/09712346/1AZL0P4dL1e/download-article/pdf","location":"Oxford, UK","rawData":"-","DOI":"10.1109/TPAMI.2022.3150906","citationCount":4,"completionStatus":"complete"},{"id":"nuPlan","href":"https://arxiv.org/abs/2106.11810","size_storage":"-","size_hours":"1500","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"8x cameras 2000x1200 10Hz, 5x lidar 20Hz, 1x imu 100Hz, 1x gnss 20Hz","recordingPerspective":"-","dataType":"Real","mapData":"No","benchmark":"autonomous vehicle planning","annotations":"2d/3d bounding boxes","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)","relatedDatasets":"-","publishDate":"2021-12-10","lastUpdate":"-","paperTitle":"nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles","relatedPaper":"https://arxiv.org/abs/2106.11810","location":"Boston, Pittsburgh, Las Vegas and Singapore","rawData":"-","DOI":"10.48550/arXiv.2106.11810","citationCount":4,"completionStatus":"complete"},{"id":"The USyd Campus Dataset","href":"http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/","size_storage":"","size_hours":"40","frames":"","numberOfScenes":"","samplingRate":"","lengthOfScenes":"","sensors":"6 cameras, lidar, gps, imu, wheel encoders","sensorDetail":"GMSL cameras 30, 60 Hz, VLP-16 10 Hz","benchmark":"","annotations":"Semantic Segmentation","licensing":"","relatedDatasets":"","publishDate":"2020-06-05","lastUpdate":"","relatedPaper":"https://ieeexplore.ieee.org/document/9109704","DOI":"10.1109/MITS.2020.2990183","citationCount":4,"completionStatus":"partially Complete"},{"id":"CODA","href":"https://coda-dataset.github.io/","relatedPaper":"https://arxiv.org/pdf/2203.07724.pdf","benchmark":"anomaly detection","citationCount":2,"completionStatus":"partially Complete"},{"id":"A9","href":"https://innovation-mobility.com/en/a9-dataset/","relatedPaper":"https://arxiv.org/pdf/2204.06527.pdf","citationCount":2,"completionStatus":"partially Complete"},{"id":"Boreas","href":"https://www.boreas.utias.utoronto.ca/#/","relatedPaper":"https://arxiv.org/pdf/2203.10168.pdf","citationCount":2,"completionStatus":"partially Complete"},{"id":"DRIV100","href":"https://zenodo.org/record/4389243#.YnvlruhBxD8","size_storage":"-","size_hours":"-","frames":"-","numberOfScenes":"100","samplingRate":"-","lengthOfScenes":"-","sensors":"camera","sensorDetail":"-","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"Domain adaptation techniques on in-the-wild road-scene videos collected from the Internet","annotations":"pixel level semantic segmentation","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License","relatedDatasets":"-","publishDate":"2021-01-30","lastUpdate":"-","paperTitle":"DRIV100: In-The-Wild Multi-Domain Dataset and Evaluation for Real-World Domain Adaptation of Semantic Segmentation","relatedPaper":"https://arxiv.org/pdf/2102.00150.pdf","location":"Random videos from YouTube","rawData":"-","DOI":"10.5281/zenodo.4389243","citationCount":1,"completionStatus":"complete"},{"id":"MIT-AVT Clustered Driving Scene Dataset","href":"https://ieeexplore.ieee.org/abstract/document/9304677/","size_storage":"4000","size_hours":"3212","frames":"-","numberOfScenes":"1156592","samplingRate":"30","lengthOfScenes":"10","sensors":"camera, imu, gps","sensorDetail":"-","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"Yes","benchmark":"-","annotations":"Seasons, Weather, Lanes, Illuminnation, Simplified Road Type, Others","licensing":"Not released publically","relatedDatasets":"-","publishDate":"2020-11-13","lastUpdate":"-","paperTitle":"MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in Real-World Naturalistic Driving Scenarios","relatedPaper":"https://ieeexplore.ieee.org/abstract/document/9304677/","location":"Many states across the USA","rawData":"Yes","DOI":"10.1109/IV47402.2020.9304677","citationCount":1,"completionStatus":"complete"},{"id":"SODA10M","href":"https://soda-2d.github.io/","relatedPaper":"https://arxiv.org/pdf/2106.11118.pdf","citationCount":1,"completionStatus":"partially Complete"},{"id":"DGL-MOTS","href":"https://goodproj13.github.io/DGL-MOTS/","relatedPaper":"https://arxiv.org/pdf/2110.07790.pdf","citationCount":1,"completionStatus":"partially Complete"},{"id":"PandaSet","href":"https://pandaset.org/","size_hours":"0.23","size_storage":"-","frames":"48000","numberOfScenes":"103","samplingRate":"-","lengthOfScenes":"8","sensors":"camera, lidar, gps/imu","sensorDetail":"5x wide angle cameras 1920x1080 10Hz, 1x long focus camera 1920x1080 10Hz, 1x mechanical spinning LiDAR 64 channels 360\xb0 10Hz, 1x forward-facing LiDAR 150 channels 60\xb0 10Hz1x mechanical spinning LiDAR, 1x forward-facing LiDAR, 6x cameras, on-board GPS/IMU","benchmark":"-","annotations":"3d bounding boxes, attributes, point cloud segmentation ","licensing":"Creative Commons Attribution 4.0 International Public (CC BY 4.0)","relatedDatasets":"-","publishDate":"2020-04-01","lastUpdate":"-","location":"San Francisco and El Camina Real, USA","rawData":"Yes","citationCount":0,"completionStatus":"partially Complete"},{"id":"Udacity","href":"https://github.com/udacity/self-driving-car/","size_storage":"223","size_hours":"10","frames":"-","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"camera, lidar, gps/imu","sensorDetail":"monocular color camera 1920x1200, velodyne 32 lidar, gps/imu","benchmark":"-","annotations":"2d bounding boxes","licensing":"MIT","relatedDatasets":"-","publishDate":"2016-09-01","lastUpdate":"-","location":"-","rawData":"True","citationCount":0,"completionStatus":"partially Complete"},{"id":"WZ-traffic dataset","href":"https://github.com/Fangyu0505/traffic-scene-recognition","citationCount":0,"completionStatus":"partially Complete"},{"id":"Nighttime Driving","href":"http://people.ee.ethz.ch/~daid/NightDriving/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Cooperative Driving Dataset (CODD)","href":"https://github.com/eduardohenriquearnold/CODD","size_hours":"-","size_storage":"16.7","frames":"13500","numberOfScenes":"-","samplingRate":"-","lengthOfScenes":"-","sensors":"lidar","sensorDetail":"-","recordingPerspective":"Top Shot","dataType":"Real","benchmark":"-","annotations":"3d bounding boxes","licensing":"Creative Commons Attribution Share Alike 4.0 International","relatedDatasets":"-","publishDate":"2021-11-23","lastUpdate":"-","paperTitle":"Fast and Robust Registration of Partially Overlapping Point Clouds","relatedPaper":"https://arxiv.org/pdf/2112.09922.pdf","location":"CARLA environment","rawData":"-","DOI":"10.1109/LRA.2021.3137888","citationCount":0,"completionStatus":"complete"},{"id":"AIODrive","href":"http://www.aiodrive.org/overview.html","size_hours":"-","size_storage":"3041.71","frames":"100000","numberOfScenes":"100","samplingRate":"-","lengthOfScenes":"100","sensors":"camera, lidar, radar, gps/imu","sensorDetail":"5x color cameras 1920x720 10Hz, 5x depth cameras 1920x720 10Hz, 3x lidar 64/800/1200 channels 360\xb0 10Hz, 1x SPAD-LiDAR, 4x radar 360\xb0 10Hz, 1x gps/imu 10Hz","recordingPerspective":"Bird\u2019s Eye","dataType":"Real","benchmark":"3d object detection, trajectory forecasting","annotations":"2d/3d bounding boxes, object category and attributes, 2d-3d semantic, instance and panoptic segmentation","licensing":"freely available for both commercial and non-commercial purposes","relatedDatasets":"-","publishDate":"2021-04-06","lastUpdate":"-","paperTitle":"All-In-One Drive: A Comprehensive Perception Dataset with High-Density Long-Range Point Clouds","relatedPaper":"https://www.xinshuoweng.com/papers/AIODrive/arXiv.pdf","location":"CARLA environment","rawData":"-","DOI":"10.13140/RG.2.2.21621.81122","citationCount":0,"completionStatus":"complete"},{"id":"Steet Hazards Dataset","href":"https://once-for-auto-driving.github.io/index.html","citationCount":0,"completionStatus":"partially Complete"},{"id":"RoadObstacle21","href":"https://segmentmeifyoucan.com/datasets","citationCount":0,"completionStatus":"partially Complete"},{"id":"Astyx Dataset","href":"https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/astyx_dataset.html","citationCount":0,"completionStatus":"partially Complete"},{"id":"A Parametric Top-View Representation of Complex Road Scenes","href":"https://www.nec-labs.com/~mas/BEV/","citationCount":0,"completionStatus":"partially Complete"},{"id":"VITRO","href":"https://vitro-testing.com/test-data/dashcam-annotations/","citationCount":0,"completionStatus":"partially Complete"},{"id":"UDrive Dataset","href":"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjqnfKIoK3uAhUOuaQKHZwcDEgQFjASegQIFBAC&url=https%3A%2F%2Ferticonetwork.com%2Fwp-content%2Fuploads%2F2017%2F12%2FUDRIVE-D41.1-UDrive-dataset-and-key-analysis-results-with-annotation-codebook.pdf&usg=AOvVaw17NgwnPrIal53hUYco9klG","citationCount":0,"completionStatus":"partially Complete"},{"id":"PolySync Dataset","href":"http://selfracingcars.com/blog/2016/7/26/polysync","citationCount":0,"completionStatus":"partially Complete"},{"id":"DriveSeg (MANUAL)","href":"https://agelab.mit.edu/driveseg","size_storage":"2.98","size_hours":"0.03","frames":"5000","numberOfScenes":"1","samplingRate":"30","lengthOfScenes":"167","sensors":"camera","sensorDetail":"1x FDR-AX53 camera 1080P 1920x1080 30Hz","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"semantic segmentation","licensing":"freely available to academic and nonacademic entities for non-commercial purposes","relatedDatasets":"DriveSeg (Semi-auto)","publishDate":"2020-04-06","lastUpdate":"-","paperTitle":"MIT DriveSeg (Manual) Dataset for Dynamic Driving Scene Segmentation","relatedPaper":"https://ieee-dataport.s3.amazonaws.com/docs/25911/MIT_DriveSeg_Semiauto.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20220522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220522T125535Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=7e2583baac0a4ad83560a6945835066bead73730d912e573c360d500a5802022","location":"-","rawData":"-","DOI":"10.21227/mmke-dv03","citationCount":0,"completionStatus":"complete"},{"id":"DriveSeg (Semi-auto)","href":"https://agelab.mit.edu/driveseg","size_storage":"13.46","size_hours":"0.186","frames":"20100","numberOfScenes":"67","samplingRate":"30","lengthOfScenes":"10","sensors":"camera","sensorDetail":"-","recordingPerspective":"ego-perspective","dataType":"Real","mapData":"No","benchmark":"-","annotations":"pixel-wise semantic annotation","licensing":"-","relatedDatasets":"DriveSeg (MANUAL), MIT-AVT Clustered Driving Scene Dataset","publishDate":"2020-04-06","lastUpdate":"-","paperTitle":"MIT DriveSeg (Semi-auto) Dataset: Large-scale Semi-automated Annotation of Semantic Driving Scenes","relatedPaper":"https://ieee-dataport.s3.amazonaws.com/docs/25911/MIT_DriveSeg_Semiauto.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20220522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220522T125535Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=7e2583baac0a4ad83560a6945835066bead73730d912e573c360d500a5802022","location":"-","rawData":"-","DOI":"10.21227/nb3n-kk46","citationCount":0,"completionStatus":"complete"},{"id":"KUL Belgium Traffic Sign dataset","href":"https://people.ee.ethz.ch/~timofter/traffic_signs/","citationCount":0,"completionStatus":"partially Complete"},{"id":"comma10k","href":"https://github.com/commaai/comma10k","citationCount":0,"completionStatus":"partially Complete"},{"id":"CULane Dataset","href":"https://xingangpan.github.io/projects/CULane.html","citationCount":0,"completionStatus":"partially Complete"},{"id":"DAVIS Driving Dataset","href":"https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub","citationCount":0,"completionStatus":"partially Complete"},{"id":"DBNet","href":"http://www.dbehavior.net/","citationCount":0,"completionStatus":"partially Complete"},{"id":"DIPLECS Autonomous Driving Datasets","href":"https://cvssp.org/data/diplecs/","citationCount":0,"completionStatus":"partially Complete"},{"id":"DR(eye)VE","href":"https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8","citationCount":0,"completionStatus":"partially Complete"},{"id":"SemanticPOSS","href":"http://www.poss.pku.edu.cn/semanticposs.html","citationCount":0,"completionStatus":"partially Complete"},{"id":"SemanticUSL","href":"https://unmannedlab.github.io/semanticusl","citationCount":0,"completionStatus":"partially Complete"},{"id":"ELEKTRA","href":"http://adas.cvc.uab.es/elektra/datasets/","citationCount":0,"completionStatus":"partially Complete"},{"id":"German Traffic Sign","href":"https://benchmark.ini.rub.de/","citationCount":0,"completionStatus":"partially Complete"},{"id":"HCI Challenging Stereo","href":"https://hci.iwr.uni-heidelberg.de/benchmarks/Challenging_Data_for_Stereo_and_Optical_Flow","citationCount":0,"completionStatus":"partially Complete"},{"id":"HD1K","href":"http://hci-benchmark.iwr.uni-heidelberg.de/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Highway Work Zones","href":"http://www.andrew.cmu.edu/user/jonghole/workzone/data/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Amodal Cityscapes","href":"https://github.com/ifnspaml/AmodalCityscapes","citationCount":0,"completionStatus":"partially Complete"},{"id":"LISA Traffic Sign Dataset","href":"http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html","citationCount":0,"completionStatus":"partially Complete"},{"id":"Malaga Stereo","href":"https://www.mrpt.org/MalagaUrbanDataset","citationCount":0,"completionStatus":"partially Complete"},{"id":"Malaga Laser Urban","href":"https://www.mrpt.org/MalagaUrbanDataset","citationCount":0,"completionStatus":"partially Complete"},{"id":"Mapillary Vistas","href":"https://www.mapillary.com/dataset/vistas","citationCount":0,"completionStatus":"partially Complete"},{"id":"NEXET","href":"https://blog.getnexar.com/https-medium-com-itayklein-intro-nexet-50e9b596d0e5","citationCount":0,"completionStatus":"partially Complete"},{"id":"Ground Truth Stixel Dataset","href":"http://www.6d-vision.com/ground-truth-stixel-dataset","citationCount":0,"completionStatus":"partially Complete"},{"id":"Boxy","href":"https://boxy-dataset.com/boxy/","citationCount":0,"completionStatus":"partially Complete"},{"id":"TME Motorway","href":"http://cmp.felk.cvut.cz/data/motorway/","citationCount":0,"completionStatus":"partially Complete"},{"id":"TuSimple","href":"https://www.tusimple.com/","citationCount":0,"completionStatus":"partially Complete"},{"id":"UAH-DriveSet","href":"http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Unsupervised Llamas","href":"https://unsupervised-llamas.com/llamas/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Dense Depth for Autonomous Driving","href":"https://github.com/TRI-ML/DDAD","citationCount":0,"completionStatus":"partially Complete"},{"id":"Canadian Adverse Driving Conditions","href":"http://cadcd.uwaterloo.ca/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Street Hazards","href":"https://github.com/hendrycks/anomaly-seg","citationCount":0,"completionStatus":"partially Complete"},{"id":"Astyx HiRes 2019","href":"https://www.astyx.com/fileadmin/redakteur/dokumente/Astyx_Dataset_HiRes2019_specification.pdf","citationCount":0,"completionStatus":"partially Complete"},{"id":"A*3D","href":"https://github.com/I2RDL2/ASTAR-3D","citationCount":0,"completionStatus":"partially Complete"},{"id":"camvid","href":"https://www.kaggle.com/carlolepelaars/camvid","citationCount":0,"completionStatus":"partially Complete"},{"id":"Daimler Urban Segmentation","href":"https://computervisiononline.com/dataset/1105138608","citationCount":0,"completionStatus":"partially Complete"},{"id":"VPGNet","href":"https://arxiv.org/abs/1710.06288","citationCount":0,"completionStatus":"partially Complete"},{"id":"Toronto 3D","href":"https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Tan_Toronto-3D_A_Large-Scale_Mobile_LiDAR_Dataset_for_Semantic_Segmentation_of_CVPRW_2020_paper.pdf","citationCount":0,"completionStatus":"partially Complete"},{"id":"Toronto City","href":"http://www.cs.toronto.edu/~byang/papers/Tcity_iccv17.pdf","citationCount":0,"completionStatus":"partially Complete"},{"id":"Synthia","href":"https://synthia-dataset.net/","citationCount":0,"completionStatus":"partially Complete"},{"id":"RANUS","href":"https://sites.google.com/site/gmchoe1/ranus","citationCount":0,"completionStatus":"partially Complete"},{"id":"One Thousand and One Hours","href":"https://arxiv.org/abs/2006.14480v2","citationCount":0,"completionStatus":"partially Complete"},{"id":"LIBRE","href":"https://arxiv.org/abs/2003.06129","citationCount":0,"completionStatus":"partially Complete"},{"id":"Stanford Track Collection","href":"https://cs.stanford.edu/people/teichman/stc/","citationCount":0,"completionStatus":"partially Complete"},{"id":"LiDAR-Video Driving Dataset","href":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf","citationCount":0,"completionStatus":"partially Complete"},{"id":"WoodScape","href":"https://paperswithcode.com/dataset/woodscape","citationCount":0,"completionStatus":"partially Complete"},{"id":"Raincouver","href":"https://ieeexplore.ieee.org/document/7970170","citationCount":0,"completionStatus":"partially Complete"},{"id":"TRoM","href":"https://ieeexplore.ieee.org/document/8317749","citationCount":0,"completionStatus":"partially Complete"},{"id":"Caltech Lanes","href":"http://www.mohamedaly.info/datasets/caltech-lanes","citationCount":0,"completionStatus":"partially Complete"},{"id":"Complex Urban Dataset","href":"https://irap.kaist.ac.kr/dataset/","citationCount":0,"completionStatus":"partially Complete"},{"id":"CCSAD","href":"https://www.researchgate.net/publication/277476726_Towards_Ubiquitous_Autonomous_Driving_The_CCSAD_Dataset","citationCount":0,"completionStatus":"partially Complete"},{"id":"Street Learn","href":"https://paperswithcode.com/dataset/streetlearn","citationCount":0,"completionStatus":"partially Complete"},{"id":"Multi Vehicle Stereo Event Camera","href":"https://daniilidis-group.github.io/mvsec/","citationCount":0,"completionStatus":"partially Complete"},{"id":"AMUSE","href":"http://www.cvl.isy.liu.se/research/datasets/amuse/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Cheddar Gorge Dataset","href":"https://www.researchgate.net/publication/228428941_The_Cheddar_Gorge_Data_Set","citationCount":0,"completionStatus":"partially Complete"},{"id":"The Annotated Laser Dataset","href":"https://journals.sagepub.com/doi/pdf/10.1177/0278364910389840","citationCount":0,"completionStatus":"partially Complete"},{"id":"DDD 17","href":"https://www.paperswithcode.com/dataset/ddd17","citationCount":0,"completionStatus":"partially Complete"},{"id":"BLVD","href":"https://paperswithcode.com/dataset/blvd","citationCount":0,"completionStatus":"partially Complete"},{"id":"FLIR Thermal Dataset","href":"https://www.flir.com/oem/adas/adas-dataset-form/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Multispectral Object Detection","href":"https://deepai.org/publication/multispectral-object-detection-with-deep-learning","citationCount":0,"completionStatus":"partially Complete"},{"id":"CityPersons","href":"https://arxiv.org/abs/1702.05693","citationCount":0,"completionStatus":"partially Complete"},{"id":"Tsinghua Daimler Cyclist Detection","href":"http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html","citationCount":0,"completionStatus":"partially Complete"},{"id":"TDU Brussels Pedestrian","href":"https://www.researchgate.net/figure/Results-on-the-TUD-Brussels-Pedestrian-Dataset_fig13_321232691","citationCount":0,"completionStatus":"partially Complete"},{"id":"ETH Pedestrian","href":"https://paperswithcode.com/dataset/eth","citationCount":0,"completionStatus":"partially Complete"},{"id":"Daimler Pedestrian","href":"http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html","citationCount":0,"completionStatus":"partially Complete"},{"id":"GROUNDED Localizing Ground Penetrating Radar (LGPR) Evaluation Dataset","href":"https://lgprdata.com/","citationCount":0,"completionStatus":"partially Complete"},{"id":"highD","href":"https://www.highd-dataset.com/","citationCount":0,"completionStatus":"partially Complete"},{"id":"inD","href":"https://www.ind-dataset.com/","citationCount":0,"completionStatus":"partially Complete"},{"id":"rounD","href":"https://www.round-dataset.com/","citationCount":0,"completionStatus":"partially Complete"},{"id":"TAF-BW","href":"https://github.com/fzi-forschungszentrum-informatik/test-area-autonomous-driving-dataset","citationCount":0,"completionStatus":"partially Complete"},{"id":"Road Scene Graph","href":"https://github.com/TianYafu/road-status-graph-dataset","citationCount":0,"completionStatus":"partially Complete"},{"id":"R3 Driving Dataset","href":"https://github.com/rllab-snu/R3-Driving-Dataset","relatedPaper":"https://arxiv.org/pdf/2109.07995.pdf","citationCount":0,"completionStatus":"partially Complete"},{"id":"EISATS","href":"https://ccv.wordpress.fos.auckland.ac.nz/eisats/","size_storage":"","size_hours":"","frames":"","numberOfScenes":"","samplingRate":"","lengthOfScenes":"","sensors":"","sensorDetail":"","benchmark":"","annotations":"","licensing":"","relatedDatasets":"","publishDate":" ","lastUpdate":"","relatedPaper":"","citationCount":0,"completionStatus":"incomplete"},{"id":"Ford CAMPUS","href":"https://www.researchgate.net/publication/220122924_Ford_Campus_vision_and_lidar_data_set","size_storage":"","size_hours":"","frames":"","numberOfScenes":"","samplingRate":"","lengthOfScenes":"","sensors":"","sensorDetail":"","benchmark":"","annotations":"","licensing":"","relatedDatasets":"","publishDate":" ","lastUpdate":"","relatedPaper":"","citationCount":0,"completionStatus":"incomplete"},{"id":"Argoverse Stereo","href":"https://www.argoverse.org/data.html#stereo-link","size_storage":"14.2","size_hours":"","frames":"","numberOfScenes":"","samplingRate":"","lengthOfScenes":"","sensors":"","sensorDetail":"","benchmark":"stereo","annotations":"","licensing":"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)","relatedDatasets":"Argoverse 3D Tracking","publishDate":"2021-04-01","lastUpdate":"","relatedPaper":"","citationCount":0,"completionStatus":"incomplete"},{"id":"uniD","href":"https://www.unid-dataset.com/","size_storage":"","size_hours":"","frames":"","numberOfScenes":"","samplingRate":"","lengthOfScenes":"","sensors":"","sensorDetail":"","benchmark":"","annotations":"","licensing":"","relatedDatasets":"","publishDate":" ","lastUpdate":"","relatedPaper":"","citationCount":0,"completionStatus":"incomplete"},{"id":"exiD","href":"https://www.exid-dataset.com/","size_storage":"","size_hours":"","frames":"","numberOfScenes":"","samplingRate":"","lengthOfScenes":"","sensors":"","sensorDetail":"","benchmark":"","annotations":"","licensing":"","relatedDatasets":"","publishDate":" ","lastUpdate":"","relatedPaper":"","citationCount":0,"completionStatus":"incomplete"},{"id":"Argoverse 2","href":"https://www.argoverse.org/av2.html","relatedPaper":"https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf","citationCount":0,"completionStatus":"incomplete"},{"id":"Synthetic Discrepancy Datasets","href":"https://github.com/cvlab-epfl/detecting-the-unexpected","relatedPaper":"https://openaccess.thecvf.com/content_ICCV_2019/papers/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.pdf","citationCount":0,"completionStatus":"incomplete"},{"id":"Rope3D","href":"https://thudair.baai.ac.cn/rope","relatedPaper":"https://arxiv.org/pdf/2203.13608.pdf","citationCount":0,"completionStatus":"partially Complete"},{"id":"OpenMPD","href":"http://openmpd.com/","relatedPaper":"https://ieeexplore.ieee.org/document/9682587","citationCount":0,"completionStatus":"incomplete"},{"id":"On Salience-Sensitive Sign Classification in Autonomous Vehicle Path Planning","relatedPaper":"https://arxiv.org/pdf/2112.00942.pdf","citationCount":0,"completionStatus":"incomplete"},{"id":"The Autonomous Platform Inertial Dataset","href":"https://github.com/ansfl/Navigation-Data-Project/","relatedPaper":"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9684368","citationCount":0,"completionStatus":"incomplete"},{"id":"PIE","href":"https://data.nvision2.eecs.yorku.ca/PIE_dataset/","citationCount":0,"completionStatus":"partially Complete"},{"id":"Gated2Gated","href":"https://github.com/princeton-computational-imaging/Gated2Gated#gated2gated--self-supervised-depth-estimation-from-gated-images","size_storage":"-","size_hours":"-","frames":"130000","numberOfScenes":"1835","samplingRate":"10","lengthOfScenes":"-","sensors":"camera, radar, lidar, imu, weather sensor","sensorDetail":"2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)","recordingPerspective":"Ego-perspective","dataType":"real","mapData":"No","benchmark":"-","annotations":"-","licensing":"Freely available for research and teaching purposes","relatedDatasets":"SeeingThroughFog, PointCloudDeNoising, Gated2Depth","publishDate":"2021-12-04","lastUpdate":"-","paperTitle":"Gated2Gated: Self-Supervised Depth Estimation from Gated Images","relatedPaper":"https://arxiv.org/pdf/2112.02416.pdf","location":"Germany, Sweden, Denmark and Finland","rawData":"-","DOI":"10.48550/arXiv.2112.02416","citationCount":0,"completionStatus":"complete"}]')}},[[146,1,2]]]);
//# sourceMappingURL=main.31c3b1e7.chunk.js.map